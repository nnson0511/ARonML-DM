{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '192.168.1.212'\n",
    "database = 'master'\n",
    "username = 'test'\n",
    "password = 'tester2024'\n",
    "\n",
    "mssql_conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        select cp.CompanyId, CompanyAge, CompanyType, FDI, CapitalAmount, NumberOfLabors, Region, Status,\n",
      "                [2015_11], [2015_12], [2015_13], [2015_14], [2015_15], [2015_16], [2015_17], [2015_18], [2015_19], [2015_20], [2015_21], [2015_22], [2015_23], [2015_24],\n",
      "                [2016_11], [2016_12], [2016_13], [2016_14], [2016_15], [2016_16], [2016_17], [2016_18], [2016_19], [2016_20], [2016_21], [2016_22], [2016_23], [2016_24],\n",
      "                [2017_11], [2017_12], [2017_13], [2017_14], [2017_15], [2017_16], [2017_17], [2017_18], [2017_19], [2017_20], [2017_21], [2017_22], [2017_23], [2017_24],\n",
      "                [2018_11], [2018_12], [2018_13], [2018_14], [2018_15], [2018_16], [2018_17], [2018_18], [2018_19], [2018_20], [2018_21], [2018_22], [2018_23], [2018_24],\n",
      "                [2019_11], [2019_12], [2019_13], [2019_14], [2019_15], [2019_16], [2019_17], [2019_18], [2019_19], [2019_20], [2019_21], [2019_22], [2019_23], [2019_24],\n",
      "                [2020_11], [2020_12], [2020_13], [2020_14], [2020_15], [2020_16], [2020_17], [2020_18], [2020_19], [2020_20], [2020_21], [2020_22], [2020_23], [2020_24],\n",
      "                [2021_11], [2021_12], [2021_13], [2021_14], [2021_15], [2021_16], [2021_17], [2021_18], [2021_19], [2021_20], [2021_21], [2021_22], [2021_23], [2021_24],\n",
      "                [2022_11], [2022_12], [2022_13], [2022_14], [2022_15], [2022_16], [2022_17], [2022_18], [2022_19], [2022_20], [2022_21], [2022_22], [2022_23], [2022_24]\n",
      "        from ProjectNew..CompanyProfile cp\n",
      "        join ProjectNew..FinancialValue2 fv on cp.CompanyId = fv.CompanyId\n",
      "        where CompanyAge > 9\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "msql_query = f\"\"\"\n",
    "        select cp.CompanyId, CompanyAge, CompanyType, FDI, CapitalAmount, NumberOfLabors, Region, Status,\n",
    "                [2015_11], [2015_12], [2015_13], [2015_14], [2015_15], [2015_16], [2015_17], [2015_18], [2015_19], [2015_20], [2015_21], [2015_22], [2015_23], [2015_24],\n",
    "                [2016_11], [2016_12], [2016_13], [2016_14], [2016_15], [2016_16], [2016_17], [2016_18], [2016_19], [2016_20], [2016_21], [2016_22], [2016_23], [2016_24],\n",
    "                [2017_11], [2017_12], [2017_13], [2017_14], [2017_15], [2017_16], [2017_17], [2017_18], [2017_19], [2017_20], [2017_21], [2017_22], [2017_23], [2017_24],\n",
    "                [2018_11], [2018_12], [2018_13], [2018_14], [2018_15], [2018_16], [2018_17], [2018_18], [2018_19], [2018_20], [2018_21], [2018_22], [2018_23], [2018_24],\n",
    "                [2019_11], [2019_12], [2019_13], [2019_14], [2019_15], [2019_16], [2019_17], [2019_18], [2019_19], [2019_20], [2019_21], [2019_22], [2019_23], [2019_24],\n",
    "                [2020_11], [2020_12], [2020_13], [2020_14], [2020_15], [2020_16], [2020_17], [2020_18], [2020_19], [2020_20], [2020_21], [2020_22], [2020_23], [2020_24],\n",
    "                [2021_11], [2021_12], [2021_13], [2021_14], [2021_15], [2021_16], [2021_17], [2021_18], [2021_19], [2021_20], [2021_21], [2021_22], [2021_23], [2021_24],\n",
    "                [2022_11], [2022_12], [2022_13], [2022_14], [2022_15], [2022_16], [2022_17], [2022_18], [2022_19], [2022_20], [2022_21], [2022_22], [2022_23], [2022_24]\n",
    "        from ProjectNew..CompanyProfile cp\n",
    "        join ProjectNew..FinancialValue2 fv on cp.CompanyId = fv.CompanyId\n",
    "        where CompanyAge > 9\n",
    "        \"\"\"\n",
    "print(msql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối cơ sở dữ liệu thành công\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nnson\\AppData\\Local\\Temp\\ipykernel_23644\\1879818091.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql_query(msql_query, mssql_conn)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mssql_conn = pyodbc.connect(mssql_conn_str)\n",
    "    print(\"Kết nối cơ sở dữ liệu thành công\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Lỗi khi kết nối cơ sở dữ liệu: {e}\")\n",
    "\n",
    "data = pd.read_sql_query(msql_query, mssql_conn)\n",
    "\n",
    "mssql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df1, n):\n",
    "    df = df1.copy()\n",
    "\n",
    "    # One-hot encoding cho cột CompanyType\n",
    "    df = pd.get_dummies(df, columns=['CompanyType'], prefix='Type')\n",
    "        \n",
    "    if 'CompanyId' in df.columns:\n",
    "        df.drop(columns=['CompanyId'], inplace=True)\n",
    "\n",
    "    # Duplicate các dòng dữ liệu có status = 1 n lần\n",
    "    if n > 1:\n",
    "        df_status_1 = df[df['status'] == 1]\n",
    "        df = pd.concat([df] + [df_status_1] * (n - 1), ignore_index=True)\n",
    "\n",
    "    # Normalize các cột còn lại với giá trị từ 0 đến 1\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    # Thay thế tất cả các giá trị NaN trong df thành 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processing_data(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyAge</th>\n",
       "      <th>FDI</th>\n",
       "      <th>CapitalAmount</th>\n",
       "      <th>NumberOfLabors</th>\n",
       "      <th>Region</th>\n",
       "      <th>Status</th>\n",
       "      <th>2015_11</th>\n",
       "      <th>2015_12</th>\n",
       "      <th>2015_13</th>\n",
       "      <th>2015_14</th>\n",
       "      <th>...</th>\n",
       "      <th>2022_19</th>\n",
       "      <th>2022_20</th>\n",
       "      <th>2022_21</th>\n",
       "      <th>2022_22</th>\n",
       "      <th>2022_23</th>\n",
       "      <th>2022_24</th>\n",
       "      <th>Type_LLC1</th>\n",
       "      <th>Type_LLC2</th>\n",
       "      <th>Type_PE</th>\n",
       "      <th>Type_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236214</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.332620</td>\n",
       "      <td>0.344626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58896</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236269</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.196137</td>\n",
       "      <td>0.332621</td>\n",
       "      <td>0.344626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58897</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58898</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236236</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.196147</td>\n",
       "      <td>0.332621</td>\n",
       "      <td>0.344627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58899</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236548</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.196325</td>\n",
       "      <td>0.332678</td>\n",
       "      <td>0.344673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58900</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236222</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.332620</td>\n",
       "      <td>0.344626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58901 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompanyAge  FDI  CapitalAmount  NumberOfLabors  Region  Status  \\\n",
       "0            0.28  0.0       0.000097        0.000076     1.0     0.0   \n",
       "1            0.08  0.0       0.000009        0.000031     1.0     0.0   \n",
       "2            0.28  0.0       0.000001        0.000138     0.0     0.0   \n",
       "3            0.08  0.0       0.000029        0.000076     1.0     0.0   \n",
       "4            0.44  0.0       0.000031        0.000000     1.0     1.0   \n",
       "...           ...  ...            ...             ...     ...     ...   \n",
       "58896        0.24  0.0       0.000097        0.000153     0.0     0.0   \n",
       "58897        0.36  0.0       0.000005        0.000015     1.0     0.0   \n",
       "58898        0.20  0.0       0.000039        0.000046     1.0     0.0   \n",
       "58899        0.44  0.0       0.000243        0.004584     0.0     0.0   \n",
       "58900        0.04  0.0       0.000117        0.000183     1.0     0.0   \n",
       "\n",
       "        2015_11   2015_12   2015_13   2015_14  ...   2022_19   2022_20  \\\n",
       "0      0.000467  0.004616  0.000714  0.000517  ...  0.000000  0.000000   \n",
       "1      0.000254  0.004240  0.000714  0.000000  ...  0.236214  0.000006   \n",
       "2      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "3      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "4      0.000290  0.004277  0.000811  0.000562  ...  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "58896  0.000000  0.000000  0.000000  0.000000  ...  0.236269  0.000047   \n",
       "58897  0.000375  0.004282  0.001035  0.000513  ...  0.000000  0.000000   \n",
       "58898  0.000000  0.000000  0.000000  0.000000  ...  0.236236  0.000023   \n",
       "58899  0.001674  0.005229  0.002323  0.000644  ...  0.236548  0.000600   \n",
       "58900  0.000000  0.000000  0.000000  0.000000  ...  0.236222  0.000011   \n",
       "\n",
       "        2022_21   2022_22   2022_23   2022_24  Type_LLC1  Type_LLC2  Type_PE  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000        0.0        0.0      0.0   \n",
       "1      0.003024  0.196133  0.332620  0.344626        0.0        1.0      0.0   \n",
       "2      0.000000  0.000000  0.000000  0.000000        1.0        0.0      0.0   \n",
       "3      0.000000  0.000000  0.000000  0.000000        0.0        0.0      0.0   \n",
       "4      0.000000  0.000000  0.000000  0.000000        0.0        1.0      0.0   \n",
       "...         ...       ...       ...       ...        ...        ...      ...   \n",
       "58896  0.003032  0.196137  0.332621  0.344626        0.0        1.0      0.0   \n",
       "58897  0.000000  0.000000  0.000000  0.000000        0.0        1.0      0.0   \n",
       "58898  0.003109  0.196147  0.332621  0.344627        0.0        1.0      0.0   \n",
       "58899  0.003298  0.196325  0.332678  0.344673        0.0        1.0      0.0   \n",
       "58900  0.003024  0.196133  0.332620  0.344626        0.0        0.0      0.0   \n",
       "\n",
       "       Type_SC  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          1.0  \n",
       "4          0.0  \n",
       "...        ...  \n",
       "58896      0.0  \n",
       "58897      0.0  \n",
       "58898      0.0  \n",
       "58899      0.0  \n",
       "58900      1.0  \n",
       "\n",
       "[58901 rows x 122 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_logistic_regression_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo mô hình Logistic Regression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Huấn luyện mô hình trên tập train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8768355827179357\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8771646859083192\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8739388794567062\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8733446519524618\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8737691001697793\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8750105800410404\n",
      "Standard Deviation of Accuracy: 0.0016392643269249711\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     10298\n",
      "         1.0       0.48      0.04      0.08      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.68      0.52      0.51     11780\n",
      "weighted avg       0.83      0.87      0.82     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     10298\n",
      "         1.0       0.48      0.04      0.08      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.68      0.52      0.51     11780\n",
      "weighted avg       0.83      0.87      0.82     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     10298\n",
      "         1.0       0.48      0.04      0.08      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.68      0.52      0.51     11780\n",
      "weighted avg       0.83      0.87      0.82     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     10298\n",
      "         1.0       0.48      0.04      0.08      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.68      0.52      0.51     11780\n",
      "weighted avg       0.83      0.87      0.82     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     10298\n",
      "         1.0       0.48      0.04      0.08      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.68      0.52      0.51     11780\n",
      "weighted avg       0.83      0.87      0.82     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình XGBoost trên tập train\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8801459977930566\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.881578947368421\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8809847198641766\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.881578947368421\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.882088285229202\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8812753795246554\n",
      "Standard Deviation of Accuracy: 0.0006640946754673247\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.94     10298\n",
      "         1.0       0.57      0.25      0.34      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.61      0.64     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.94     10298\n",
      "         1.0       0.57      0.25      0.34      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.61      0.64     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.94     10298\n",
      "         1.0       0.57      0.25      0.34      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.61      0.64     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.94     10298\n",
      "         1.0       0.57      0.25      0.34      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.61      0.64     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.94     10298\n",
      "         1.0       0.57      0.25      0.34      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.61      0.64     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def train_lightgbm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình LightGBM trên tập train\n",
    "        model = lgb.LGBMClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "[LightGBM] [Info] Number of positive: 5842, number of negative: 41278\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29107\n",
      "[LightGBM] [Info] Number of data points in the train set: 47120, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123981 -> initscore=-1.955256\n",
      "[LightGBM] [Info] Start training from score -1.955256\n",
      "Accuracy for fold 1: 0.8845598845598845\n",
      "Training on fold 2...\n",
      "[LightGBM] [Info] Number of positive: 5857, number of negative: 41264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29107\n",
      "[LightGBM] [Info] Number of data points in the train set: 47121, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124297 -> initscore=-1.952353\n",
      "[LightGBM] [Info] Start training from score -1.952353\n",
      "Accuracy for fold 2: 0.8840407470288625\n",
      "Training on fold 3...\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 41268\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29107\n",
      "[LightGBM] [Info] Number of data points in the train set: 47121, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124212 -> initscore=-1.953133\n",
      "[LightGBM] [Info] Start training from score -1.953133\n",
      "Accuracy for fold 3: 0.8826825127334466\n",
      "Training on fold 4...\n",
      "[LightGBM] [Info] Number of positive: 5836, number of negative: 41285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29107\n",
      "[LightGBM] [Info] Number of data points in the train set: 47121, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123851 -> initscore=-1.956454\n",
      "[LightGBM] [Info] Start training from score -1.956454\n",
      "Accuracy for fold 4: 0.8831918505942276\n",
      "Training on fold 5...\n",
      "[LightGBM] [Info] Number of positive: 5820, number of negative: 41301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29107\n",
      "[LightGBM] [Info] Number of data points in the train set: 47121, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123512 -> initscore=-1.959586\n",
      "[LightGBM] [Info] Start training from score -1.959586\n",
      "Accuracy for fold 5: 0.8805602716468591\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.883007053312656\n",
      "Standard Deviation of Accuracy: 0.0013860753748771162\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94     10298\n",
      "         1.0       0.59      0.16      0.25      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94     10298\n",
      "         1.0       0.59      0.16      0.25      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94     10298\n",
      "         1.0       0.59      0.16      0.25      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94     10298\n",
      "         1.0       0.59      0.16      0.25      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94     10298\n",
      "         1.0       0.59      0.16      0.25      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_lightgbm_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def train_catboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "    \n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình CatBoost trên tập train\n",
    "        model = cb.CatBoostClassifier(verbose=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8863424157541805\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.884125636672326\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.883955857385399\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8847198641765704\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8863327674023769\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8850953082781705\n",
      "Standard Deviation of Accuracy: 0.0010455765657709426\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.62      0.25      0.36      1482\n",
      "\n",
      "    accuracy                           0.89     11780\n",
      "   macro avg       0.76      0.61      0.65     11780\n",
      "weighted avg       0.87      0.89      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.62      0.25      0.36      1482\n",
      "\n",
      "    accuracy                           0.89     11780\n",
      "   macro avg       0.76      0.61      0.65     11780\n",
      "weighted avg       0.87      0.89      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.62      0.25      0.36      1482\n",
      "\n",
      "    accuracy                           0.89     11780\n",
      "   macro avg       0.76      0.61      0.65     11780\n",
      "weighted avg       0.87      0.89      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.62      0.25      0.36      1482\n",
      "\n",
      "    accuracy                           0.89     11780\n",
      "   macro avg       0.76      0.61      0.65     11780\n",
      "weighted avg       0.87      0.89      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.62      0.25      0.36      1482\n",
      "\n",
      "    accuracy                           0.89     11780\n",
      "   macro avg       0.76      0.61      0.65     11780\n",
      "weighted avg       0.87      0.89      0.86     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_catboost_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_hist_gradient_boosting_kfold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "def train_hist_gradient_boosting_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình HistGradientBoostingClassifier trên tập train\n",
    "        model = HistGradientBoostingClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8829471182412358\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.883955857385399\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8833616298811545\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.881578947368421\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.879881154499151\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8823449414750723\n",
      "Standard Deviation of Accuracy: 0.001459336525757947\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94     10298\n",
      "         1.0       0.62      0.12      0.20      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.75      0.55      0.57     11780\n",
      "weighted avg       0.85      0.88      0.84     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94     10298\n",
      "         1.0       0.62      0.12      0.20      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.75      0.55      0.57     11780\n",
      "weighted avg       0.85      0.88      0.84     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94     10298\n",
      "         1.0       0.62      0.12      0.20      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.75      0.55      0.57     11780\n",
      "weighted avg       0.85      0.88      0.84     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94     10298\n",
      "         1.0       0.62      0.12      0.20      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.75      0.55      0.57     11780\n",
      "weighted avg       0.85      0.88      0.84     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94     10298\n",
      "         1.0       0.62      0.12      0.20      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.75      0.55      0.57     11780\n",
      "weighted avg       0.85      0.88      0.84     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_hist_gradient_boosting_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8812494694847636\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8804753820033956\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8804753820033956\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8786926994906621\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8828522920203735\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8807490450005181\n",
      "Standard Deviation of Accuracy: 0.001345541706440657\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.59      0.22      0.32      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.60      0.63     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.59      0.22      0.32      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.60      0.63     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.59      0.22      0.32      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.60      0.63     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.59      0.22      0.32      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.60      0.63     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     10298\n",
      "         1.0       0.59      0.22      0.32      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.74      0.60      0.63     11780\n",
      "weighted avg       0.86      0.88      0.86     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_RandomForest_kfold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_RandomForest_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train đã được resample\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8702996350055173\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8695246179966044\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8699490662139219\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8678268251273344\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.866044142614601\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8687288573915957\n",
      "Standard Deviation of Accuracy: 0.0015881336507073622\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92     10298\n",
      "         1.0       0.46      0.37      0.41      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.69      0.65      0.67     11780\n",
      "weighted avg       0.86      0.87      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92     10298\n",
      "         1.0       0.46      0.37      0.41      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.69      0.65      0.67     11780\n",
      "weighted avg       0.86      0.87      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92     10298\n",
      "         1.0       0.46      0.37      0.41      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.69      0.65      0.67     11780\n",
      "weighted avg       0.86      0.87      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92     10298\n",
      "         1.0       0.46      0.37      0.41      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.69      0.65      0.67     11780\n",
      "weighted avg       0.86      0.87      0.86     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92     10298\n",
      "         1.0       0.46      0.37      0.41      1482\n",
      "\n",
      "    accuracy                           0.87     11780\n",
      "   macro avg       0.69      0.65      0.67     11780\n",
      "weighted avg       0.86      0.87      0.86     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_smote_RandomForest_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_class_weight_kfold\n",
    "\n",
    "# Imbalanced Learning Techniques - Class Weight Adjustment: Điều chỉnh trọng số lớp để mô hình tập trung hơn vào lớp thiểu số.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest_class_weight_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier với class_weight='balanced'\n",
    "        model = RandomForestClassifier(class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8759018759018758\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8801358234295416\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8788624787775892\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8771646859083192\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8774193548387097\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.877896843771207\n",
      "Standard Deviation of Accuracy: 0.0014621510518750185\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     10298\n",
      "         1.0       0.55      0.15      0.24      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.72      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     10298\n",
      "         1.0       0.55      0.15      0.24      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.72      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     10298\n",
      "         1.0       0.55      0.15      0.24      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.72      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     10298\n",
      "         1.0       0.55      0.15      0.24      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.72      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     10298\n",
      "         1.0       0.55      0.15      0.24      1482\n",
      "\n",
      "    accuracy                           0.88     11780\n",
      "   macro avg       0.72      0.57      0.59     11780\n",
      "weighted avg       0.85      0.88      0.85     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_class_weight_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_grid_search\n",
    "\n",
    "# Hyperparameter Tuning - Grid Search\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "def train_random_forest_grid_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Grid Search với RandomForestClassifier\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Huấn luyện Grid Search trên toàn bộ dữ liệu\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Grid Search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_random_forest_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mtrain_random_forest_grid_search\u001b[1;34m(processed_data, k)\u001b[0m\n\u001b[0;32m     28\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mRandomForestClassifier(), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mkf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Huấn luyện Grid Search trên toàn bộ dữ liệu\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Lấy mô hình tốt nhất từ Grid Search\u001b[39;00m\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_random_forest_grid_search(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_random_search\n",
    "\n",
    "# Hyperparameter Tuning - Random Search\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "def train_random_forest_random_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Random Search\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5)\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Random Search với RandomForestClassifier\n",
    "    random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, n_iter=100, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "    # Huấn luyện Random Search trên toàn bộ dữ liệu\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Random Search\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", random_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", random_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best parameters found:  {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 165}\n",
      "Best cross-validation accuracy:  0.8025410248150482\n",
      "Classification Report for best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.18      0.31     48177\n",
      "         1.0       0.83      1.00      0.90    187477\n",
      "\n",
      "    accuracy                           0.83    235654\n",
      "   macro avg       0.88      0.59      0.60    235654\n",
      "weighted avg       0.85      0.83      0.78    235654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_random_search(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stacking_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_stacking_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in tqdm(kf.split(X), total=k, desc=\"K-Fold Progress\"):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo các mô hình cơ bản\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('gb', GradientBoostingClassifier())\n",
    "        ]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình StackingClassifier trên tập train\n",
    "        model = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LogisticRegression()\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:   0%|          | 0/5 [01:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_stacking_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 38\u001b[0m, in \u001b[0;36mtrain_stacking_kfold\u001b[1;34m(processed_data, k)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Khởi tạo và huấn luyện mô hình StackingClassifier trên tập train\u001b[39;00m\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[0;32m     35\u001b[0m     estimators\u001b[38;5;241m=\u001b[39mestimators,\n\u001b[0;32m     36\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression()\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Dự đoán trên tập test\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:672\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    671\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:264\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m--> 264\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_val_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    282\u001b[0m     meth\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1367\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1367\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1368\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1369\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_stacking_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_deep_learning_kfold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_smote_deep_learning_kfold(processed_data, k=5, epochs=50, batch_size=32):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)        \n",
    "\n",
    "        # Xây dựng mô hình\n",
    "        model = build_model(X_train.shape[1])\n",
    "        \n",
    "        # Đào tạo mô hình với ProgbarLogger\n",
    "        model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=1, \n",
    "                  validation_data=(X_test, y_test), callbacks=[ProgbarLogger()])\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 949us/step - accuracy: 0.7307 - loss: 0.5550 - val_accuracy: 0.8077 - val_loss: 0.4823\n",
      "Epoch 2/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 903us/step - accuracy: 0.7486 - loss: 0.5218 - val_accuracy: 0.7702 - val_loss: 0.5370\n",
      "Epoch 3/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 862us/step - accuracy: 0.7481 - loss: 0.5178 - val_accuracy: 0.8035 - val_loss: 0.5049\n",
      "Epoch 4/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - accuracy: 0.7464 - loss: 0.5159 - val_accuracy: 0.7073 - val_loss: 0.5918\n",
      "Epoch 5/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - accuracy: 0.7464 - loss: 0.5155 - val_accuracy: 0.7913 - val_loss: 0.5198\n",
      "Epoch 6/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - accuracy: 0.7504 - loss: 0.5091 - val_accuracy: 0.7558 - val_loss: 0.5336\n",
      "Epoch 7/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 849us/step - accuracy: 0.7507 - loss: 0.5088 - val_accuracy: 0.7873 - val_loss: 0.5282\n",
      "Epoch 8/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827us/step - accuracy: 0.7528 - loss: 0.5056 - val_accuracy: 0.7414 - val_loss: 0.5578\n",
      "Epoch 9/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 864us/step - accuracy: 0.7509 - loss: 0.5071 - val_accuracy: 0.7571 - val_loss: 0.5185\n",
      "Epoch 10/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 832us/step - accuracy: 0.7530 - loss: 0.5040 - val_accuracy: 0.7890 - val_loss: 0.4978\n",
      "Epoch 11/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 862us/step - accuracy: 0.7536 - loss: 0.5025 - val_accuracy: 0.7265 - val_loss: 0.5692\n",
      "Epoch 12/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 846us/step - accuracy: 0.7570 - loss: 0.4955 - val_accuracy: 0.8092 - val_loss: 0.4537\n",
      "Epoch 13/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 854us/step - accuracy: 0.7552 - loss: 0.4980 - val_accuracy: 0.7670 - val_loss: 0.5299\n",
      "Epoch 14/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 819us/step - accuracy: 0.7594 - loss: 0.4956 - val_accuracy: 0.7865 - val_loss: 0.5061\n",
      "Epoch 15/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 841us/step - accuracy: 0.7572 - loss: 0.4940 - val_accuracy: 0.8021 - val_loss: 0.4903\n",
      "Epoch 16/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 982us/step - accuracy: 0.7599 - loss: 0.4907 - val_accuracy: 0.7761 - val_loss: 0.4937\n",
      "Epoch 17/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 953us/step - accuracy: 0.7603 - loss: 0.4888 - val_accuracy: 0.7929 - val_loss: 0.4763\n",
      "Epoch 18/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7630 - loss: 0.4864 - val_accuracy: 0.8044 - val_loss: 0.4988\n",
      "Epoch 19/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - accuracy: 0.7627 - loss: 0.4855 - val_accuracy: 0.7138 - val_loss: 0.6138\n",
      "Epoch 20/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 849us/step - accuracy: 0.7632 - loss: 0.4861 - val_accuracy: 0.7860 - val_loss: 0.4993\n",
      "Epoch 21/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 926us/step - accuracy: 0.7652 - loss: 0.4824 - val_accuracy: 0.7196 - val_loss: 0.5508\n",
      "Epoch 22/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 870us/step - accuracy: 0.7666 - loss: 0.4795 - val_accuracy: 0.7917 - val_loss: 0.4949\n",
      "Epoch 23/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - accuracy: 0.7643 - loss: 0.4801 - val_accuracy: 0.8144 - val_loss: 0.4503\n",
      "Epoch 24/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 990us/step - accuracy: 0.7652 - loss: 0.4760 - val_accuracy: 0.7573 - val_loss: 0.5253\n",
      "Epoch 25/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 863us/step - accuracy: 0.7665 - loss: 0.4780 - val_accuracy: 0.7719 - val_loss: 0.5108\n",
      "Epoch 26/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 870us/step - accuracy: 0.7697 - loss: 0.4728 - val_accuracy: 0.8101 - val_loss: 0.4618\n",
      "Epoch 27/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 889us/step - accuracy: 0.7679 - loss: 0.4739 - val_accuracy: 0.7679 - val_loss: 0.5396\n",
      "Epoch 28/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 900us/step - accuracy: 0.7676 - loss: 0.4745 - val_accuracy: 0.8066 - val_loss: 0.4704\n",
      "Epoch 29/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 883us/step - accuracy: 0.7712 - loss: 0.4692 - val_accuracy: 0.8089 - val_loss: 0.4683\n",
      "Epoch 30/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 925us/step - accuracy: 0.7684 - loss: 0.4684 - val_accuracy: 0.8121 - val_loss: 0.4484\n",
      "Epoch 31/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 933us/step - accuracy: 0.7727 - loss: 0.4659 - val_accuracy: 0.7781 - val_loss: 0.5024\n",
      "Epoch 32/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 936us/step - accuracy: 0.7713 - loss: 0.4663 - val_accuracy: 0.7326 - val_loss: 0.5612\n",
      "Epoch 33/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 938us/step - accuracy: 0.7729 - loss: 0.4639 - val_accuracy: 0.7906 - val_loss: 0.4848\n",
      "Epoch 34/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 917us/step - accuracy: 0.7733 - loss: 0.4636 - val_accuracy: 0.8214 - val_loss: 0.4473\n",
      "Epoch 35/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 946us/step - accuracy: 0.7755 - loss: 0.4615 - val_accuracy: 0.7784 - val_loss: 0.4899\n",
      "Epoch 36/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 885us/step - accuracy: 0.7739 - loss: 0.4608 - val_accuracy: 0.7528 - val_loss: 0.5214\n",
      "Epoch 37/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 898us/step - accuracy: 0.7752 - loss: 0.4570 - val_accuracy: 0.7475 - val_loss: 0.5487\n",
      "Epoch 38/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - accuracy: 0.7759 - loss: 0.4568 - val_accuracy: 0.7788 - val_loss: 0.5023\n",
      "Epoch 39/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 915us/step - accuracy: 0.7758 - loss: 0.4562 - val_accuracy: 0.7554 - val_loss: 0.5362\n",
      "Epoch 40/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 897us/step - accuracy: 0.7780 - loss: 0.4537 - val_accuracy: 0.7743 - val_loss: 0.5063\n",
      "Epoch 41/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - accuracy: 0.7799 - loss: 0.4545 - val_accuracy: 0.8010 - val_loss: 0.4394\n",
      "Epoch 42/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 880us/step - accuracy: 0.7795 - loss: 0.4534 - val_accuracy: 0.7621 - val_loss: 0.5207\n",
      "Epoch 43/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 918us/step - accuracy: 0.7805 - loss: 0.4512 - val_accuracy: 0.7962 - val_loss: 0.4774\n",
      "Epoch 44/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 865us/step - accuracy: 0.7809 - loss: 0.4480 - val_accuracy: 0.7802 - val_loss: 0.4824\n",
      "Epoch 45/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 897us/step - accuracy: 0.7822 - loss: 0.4454 - val_accuracy: 0.7761 - val_loss: 0.5027\n",
      "Epoch 46/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881us/step - accuracy: 0.7850 - loss: 0.4449 - val_accuracy: 0.7316 - val_loss: 0.5599\n",
      "Epoch 47/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 891us/step - accuracy: 0.7827 - loss: 0.4458 - val_accuracy: 0.7796 - val_loss: 0.4775\n",
      "Epoch 48/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 894us/step - accuracy: 0.7866 - loss: 0.4405 - val_accuracy: 0.7962 - val_loss: 0.4783\n",
      "Epoch 49/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 861us/step - accuracy: 0.7822 - loss: 0.4455 - val_accuracy: 0.7617 - val_loss: 0.5138\n",
      "Epoch 50/50\n",
      "\u001b[1m2580/2580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 905us/step - accuracy: 0.7826 - loss: 0.4442 - val_accuracy: 0.7507 - val_loss: 0.5213\n",
      "\u001b[1m369/369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step\n",
      "Accuracy for fold 1: 0.7507002801120448\n",
      "Training on fold 2...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 890us/step - accuracy: 0.7324 - loss: 0.5553 - val_accuracy: 0.7912 - val_loss: 0.5058\n",
      "Epoch 2/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 820us/step - accuracy: 0.7467 - loss: 0.5265 - val_accuracy: 0.7766 - val_loss: 0.5110\n",
      "Epoch 3/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 845us/step - accuracy: 0.7506 - loss: 0.5180 - val_accuracy: 0.7554 - val_loss: 0.5361\n",
      "Epoch 4/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 844us/step - accuracy: 0.7491 - loss: 0.5197 - val_accuracy: 0.6829 - val_loss: 0.6210\n",
      "Epoch 5/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 821us/step - accuracy: 0.7530 - loss: 0.5128 - val_accuracy: 0.7882 - val_loss: 0.5044\n",
      "Epoch 6/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 842us/step - accuracy: 0.7509 - loss: 0.5135 - val_accuracy: 0.7482 - val_loss: 0.5462\n",
      "Epoch 7/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 820us/step - accuracy: 0.7552 - loss: 0.5089 - val_accuracy: 0.7930 - val_loss: 0.4814\n",
      "Epoch 8/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 805us/step - accuracy: 0.7555 - loss: 0.5049 - val_accuracy: 0.7868 - val_loss: 0.5047\n",
      "Epoch 9/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - accuracy: 0.7540 - loss: 0.5067 - val_accuracy: 0.7666 - val_loss: 0.5097\n",
      "Epoch 10/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 835us/step - accuracy: 0.7519 - loss: 0.5076 - val_accuracy: 0.7982 - val_loss: 0.4624\n",
      "Epoch 11/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 811us/step - accuracy: 0.7557 - loss: 0.5015 - val_accuracy: 0.8023 - val_loss: 0.4608\n",
      "Epoch 12/50\n",
      "\u001b[1m2579/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - accuracy: 0.7585 - loss: 0.4983 - val_accuracy: 0.7887 - val_loss: 0.4844\n",
      "Epoch 13/50\n",
      "\u001b[1m2433/2579\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.7557 - loss: 0.4982"
     ]
    }
   ],
   "source": [
    "train_smote_deep_learning_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_passive_aggressive_kfold\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_passive_aggressive_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in tqdm(kf.split(X), total=k, desc=\"K-Fold Progress\"):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)        \n",
    "\n",
    "        # Khởi tạo và huấn luyện mô hình PassiveAggressiveClassifier trên tập train\n",
    "        model = PassiveAggressiveClassifier()\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  20%|██        | 1/5 [00:02<00:09,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 1: 0.6528399567163863\n",
      "Training on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  40%|████      | 2/5 [00:05<00:07,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2: 0.34204663597207785\n",
      "Training on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  60%|██████    | 3/5 [00:07<00:05,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 3: 0.7936602236320044\n",
      "Training on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  80%|████████  | 4/5 [00:10<00:02,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 4: 0.4894867496976512\n",
      "Training on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|██████████| 5/5 [00:12<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 5: 0.2532569488648419\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.5062581029765922\n",
      "Standard Deviation of Accuracy: 0.19765752551900564\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.07      0.13     37504\n",
      "         1.0       0.21      0.98      0.35      9626\n",
      "\n",
      "    accuracy                           0.25     47130\n",
      "   macro avg       0.57      0.52      0.24     47130\n",
      "weighted avg       0.78      0.25      0.17     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.07      0.13     37504\n",
      "         1.0       0.21      0.98      0.35      9626\n",
      "\n",
      "    accuracy                           0.25     47130\n",
      "   macro avg       0.57      0.52      0.24     47130\n",
      "weighted avg       0.78      0.25      0.17     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.07      0.13     37504\n",
      "         1.0       0.21      0.98      0.35      9626\n",
      "\n",
      "    accuracy                           0.25     47130\n",
      "   macro avg       0.57      0.52      0.24     47130\n",
      "weighted avg       0.78      0.25      0.17     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.07      0.13     37504\n",
      "         1.0       0.21      0.98      0.35      9626\n",
      "\n",
      "    accuracy                           0.25     47130\n",
      "   macro avg       0.57      0.52      0.24     47130\n",
      "weighted avg       0.78      0.25      0.17     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.07      0.13     37504\n",
      "         1.0       0.21      0.98      0.35      9626\n",
      "\n",
      "    accuracy                           0.25     47130\n",
      "   macro avg       0.57      0.52      0.24     47130\n",
      "weighted avg       0.78      0.25      0.17     47130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_smote_passive_aggressive_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_ridge_classifier_kfold\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_ridge_classifier_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in tqdm(kf.split(X), total=k, desc=\"K-Fold Progress\"):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RidgeClassifier trên tập train đã áp dụng SMOTE\n",
    "        model = RidgeClassifier()\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  20%|██        | 1/5 [00:02<00:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 1: 0.5333856697290531\n",
      "Training on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  40%|████      | 2/5 [00:04<00:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2: 0.5286753941142772\n",
      "Training on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  60%|██████    | 3/5 [00:06<00:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 3: 0.5340858458339521\n",
      "Training on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  80%|████████  | 4/5 [00:08<00:02,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 4: 0.5239651184995013\n",
      "Training on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|██████████| 5/5 [00:10<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 5: 0.5352217271377042\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.5310667510628976\n",
      "Standard Deviation of Accuracy: 0.00419303985958335\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.51      0.64     37504\n",
      "         1.0       0.25      0.63      0.35      9626\n",
      "\n",
      "    accuracy                           0.54     47130\n",
      "   macro avg       0.54      0.57      0.50     47130\n",
      "weighted avg       0.72      0.54      0.58     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.51      0.64     37504\n",
      "         1.0       0.25      0.63      0.35      9626\n",
      "\n",
      "    accuracy                           0.54     47130\n",
      "   macro avg       0.54      0.57      0.50     47130\n",
      "weighted avg       0.72      0.54      0.58     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.51      0.64     37504\n",
      "         1.0       0.25      0.63      0.35      9626\n",
      "\n",
      "    accuracy                           0.54     47130\n",
      "   macro avg       0.54      0.57      0.50     47130\n",
      "weighted avg       0.72      0.54      0.58     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.51      0.64     37504\n",
      "         1.0       0.25      0.63      0.35      9626\n",
      "\n",
      "    accuracy                           0.54     47130\n",
      "   macro avg       0.54      0.57      0.50     47130\n",
      "weighted avg       0.72      0.54      0.58     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.51      0.64     37504\n",
      "         1.0       0.25      0.63      0.35      9626\n",
      "\n",
      "    accuracy                           0.54     47130\n",
      "   macro avg       0.54      0.57      0.50     47130\n",
      "weighted avg       0.72      0.54      0.58     47130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_smote_ridge_classifier_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_random_forest_class_weight_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_random_forest_class_weight_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train đã được resample\n",
    "        model = RandomForestClassifier(class_weight='balanced',random_state=42)\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7178502471833825\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7186352931191785\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7219664339818803\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.717998769387452\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7200721408869085\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7193045769117604\n",
      "Standard Deviation of Accuracy: 0.0015454958144291086\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82     37504\n",
      "         1.0       0.35      0.45      0.40      9626\n",
      "\n",
      "    accuracy                           0.72     47130\n",
      "   macro avg       0.60      0.62      0.61     47130\n",
      "weighted avg       0.75      0.72      0.73     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82     37504\n",
      "         1.0       0.35      0.45      0.40      9626\n",
      "\n",
      "    accuracy                           0.72     47130\n",
      "   macro avg       0.60      0.62      0.61     47130\n",
      "weighted avg       0.75      0.72      0.73     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82     37504\n",
      "         1.0       0.35      0.45      0.40      9626\n",
      "\n",
      "    accuracy                           0.72     47130\n",
      "   macro avg       0.60      0.62      0.61     47130\n",
      "weighted avg       0.75      0.72      0.73     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82     37504\n",
      "         1.0       0.35      0.45      0.40      9626\n",
      "\n",
      "    accuracy                           0.72     47130\n",
      "   macro avg       0.60      0.62      0.61     47130\n",
      "weighted avg       0.75      0.72      0.73     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82     37504\n",
      "         1.0       0.35      0.45      0.40      9626\n",
      "\n",
      "    accuracy                           0.72     47130\n",
      "   macro avg       0.60      0.62      0.61     47130\n",
      "weighted avg       0.75      0.72      0.73     47130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_smote_random_forest_class_weight_kfold (processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_bayesian_glm_kfold\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "def train_bayesian_glm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình Bayesian GLM (BayesianRidge) trên tập train\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Chuyển đổi thành nhãn nhị phân\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7945725743141456\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7955061424540112\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7956546646580807\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7953364027922174\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7954593677063442\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7953058303849597\n",
      "Standard Deviation of Accuracy: 0.00038052151624859664\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     37504\n",
      "         1.0       0.46      0.01      0.02      9626\n",
      "\n",
      "    accuracy                           0.80     47130\n",
      "   macro avg       0.63      0.50      0.45     47130\n",
      "weighted avg       0.73      0.80      0.71     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     37504\n",
      "         1.0       0.46      0.01      0.02      9626\n",
      "\n",
      "    accuracy                           0.80     47130\n",
      "   macro avg       0.63      0.50      0.45     47130\n",
      "weighted avg       0.73      0.80      0.71     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     37504\n",
      "         1.0       0.46      0.01      0.02      9626\n",
      "\n",
      "    accuracy                           0.80     47130\n",
      "   macro avg       0.63      0.50      0.45     47130\n",
      "weighted avg       0.73      0.80      0.71     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     37504\n",
      "         1.0       0.46      0.01      0.02      9626\n",
      "\n",
      "    accuracy                           0.80     47130\n",
      "   macro avg       0.63      0.50      0.45     47130\n",
      "weighted avg       0.73      0.80      0.71     47130\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89     37504\n",
      "         1.0       0.46      0.01      0.02      9626\n",
      "\n",
      "    accuracy                           0.80     47130\n",
      "   macro avg       0.63      0.50      0.45     47130\n",
      "weighted avg       0.73      0.80      0.71     47130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_bayesian_glm_kfold (processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_svm_kfold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train_svm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình SVM trên tập train\n",
    "        model = SVC(probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_svm_kfold(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_automl_kfold with tpot\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "def train_automl_kfold(processed_data, k=5, generations=50, population_size=50):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình TPOTClassifier trên tập train\n",
    "        model = TPOTClassifier(generations=generations, population_size=population_size, verbosity=2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_automl_kfold(processed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
