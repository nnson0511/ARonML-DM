{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '192.168.1.212'\n",
    "database = 'master'\n",
    "username = 'test'\n",
    "password = 'tester2024'\n",
    "\n",
    "mssql_conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        select *\n",
      "        from ProjectNew..FullCompanyInfo\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "msql_query = f\"\"\"\n",
    "        select *\n",
    "        from ProjectNew..FullCompanyInfo\n",
    "        \"\"\"\n",
    "print(msql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối cơ sở dữ liệu thành công\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nnson\\AppData\\Local\\Temp\\ipykernel_18816\\1879818091.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql_query(msql_query, mssql_conn)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mssql_conn = pyodbc.connect(mssql_conn_str)\n",
    "    print(\"Kết nối cơ sở dữ liệu thành công\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Lỗi khi kết nối cơ sở dữ liệu: {e}\")\n",
    "\n",
    "data = pd.read_sql_query(msql_query, mssql_conn)\n",
    "\n",
    "mssql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df1):\n",
    "    df = df1.copy()\n",
    "\n",
    "    # One-hot encoding cho cột CompanyType\n",
    "    df = pd.get_dummies(df, columns=['CompanyType'], prefix='Type')\n",
    "        \n",
    "    if 'CompanyId' in df.columns:\n",
    "        df.drop(columns=['CompanyId'], inplace=True)\n",
    "\n",
    "    # for id in range(11, 25):\n",
    "    #     mean_col_name = f'{id}_mean'\n",
    "        \n",
    "    #     # Tìm tất cả các cột tương ứng với id hiện tại\n",
    "    #     cols_to_avg = [f'{year}_{id}' for year in range(2015, 2022)]\n",
    "        \n",
    "    #     # Tính giá trị trung bình và thêm vào cột mới\n",
    "    #     df[mean_col_name] = df[cols_to_avg].mean(axis=1)\n",
    "\n",
    "    # # Xóa các cột cũ\n",
    "    # df.drop(columns=[f'{year}_{id}' for year in range(2015, 2022) for id in range(11, 25)], inplace=True)\n",
    "\n",
    "    # Normalize các cột còn lại với giá trị từ 0 đến 1\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    # Thay thế tất cả các giá trị NaN trong df thành 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processing_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyAge</th>\n",
       "      <th>FDI</th>\n",
       "      <th>CapitalAmount</th>\n",
       "      <th>NumberOfLabors</th>\n",
       "      <th>Region</th>\n",
       "      <th>FS11</th>\n",
       "      <th>FS12</th>\n",
       "      <th>FS13</th>\n",
       "      <th>FS14</th>\n",
       "      <th>FS15</th>\n",
       "      <th>...</th>\n",
       "      <th>FS20</th>\n",
       "      <th>FS21</th>\n",
       "      <th>FS22</th>\n",
       "      <th>FS23</th>\n",
       "      <th>FS24</th>\n",
       "      <th>Status</th>\n",
       "      <th>Type_LLC1</th>\n",
       "      <th>Type_LLC2</th>\n",
       "      <th>Type_PE</th>\n",
       "      <th>Type_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071832</td>\n",
       "      <td>0.077137</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135614</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071798</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071794</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135629</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071799</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235649</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235650</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235651</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066642</td>\n",
       "      <td>0.096372</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.061944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071841</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.182202</td>\n",
       "      <td>0.217624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235652</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066632</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071796</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235653</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235654 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CompanyAge  FDI  CapitalAmount  NumberOfLabors  Region      FS11  \\\n",
       "0         0.176471  0.0       0.000292        0.000061     1.0  0.066675   \n",
       "1         0.382353  0.0       0.000097        0.000076     0.0  0.066634   \n",
       "2         0.147059  0.0       0.000015        0.000046     1.0  0.066630   \n",
       "3         0.147059  0.0       0.000048        0.000015     1.0  0.066635   \n",
       "4         0.088235  0.0       0.000002        0.000153     1.0  0.066627   \n",
       "...            ...  ...            ...             ...     ...       ...   \n",
       "235649    0.235294  0.0       0.000049        0.000153     1.0  0.066638   \n",
       "235650    0.117647  0.0       0.000005        0.000015     1.0  0.066627   \n",
       "235651    0.205882  0.0       0.000243        0.000153     1.0  0.066642   \n",
       "235652    0.088235  0.0       0.000024        0.000015     1.0  0.066632   \n",
       "235653    0.088235  0.0       0.000046        0.000076     1.0  0.066626   \n",
       "\n",
       "            FS12      FS13      FS14      FS15  ...      FS20      FS21  \\\n",
       "0       0.000000  0.135581  0.023858  0.061939  ...  0.071832  0.077137   \n",
       "1       0.000000  0.135614  0.023858  0.061955  ...  0.071798  0.077136   \n",
       "2       0.000000  0.135581  0.023858  0.061939  ...  0.071794  0.077136   \n",
       "3       0.000000  0.135629  0.023858  0.061945  ...  0.071799  0.077144   \n",
       "4       0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "235649  0.000000  0.000000  0.000000  0.000000  ...  0.071801  0.000000   \n",
       "235650  0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "235651  0.096372  0.135581  0.023917  0.061944  ...  0.071841  0.077144   \n",
       "235652  0.096354  0.135581  0.023858  0.061939  ...  0.071796  0.077136   \n",
       "235653  0.000000  0.135581  0.023858  0.061939  ...  0.071791  0.077136   \n",
       "\n",
       "            FS22      FS23      FS24  Status  Type_LLC1  Type_LLC2  Type_PE  \\\n",
       "0       0.033333  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "1       0.033332  0.182198  0.217620     1.0        0.0        0.0      0.0   \n",
       "2       0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "3       0.033334  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "4       0.033332  0.182198  0.217620     0.0        1.0        0.0      0.0   \n",
       "...          ...       ...       ...     ...        ...        ...      ...   \n",
       "235649  0.000000  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235650  0.033332  0.182198  0.217620     1.0        0.0        1.0      0.0   \n",
       "235651  0.033339  0.182202  0.217624     1.0        1.0        0.0      0.0   \n",
       "235652  0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235653  0.033332  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "\n",
       "        Type_SC  \n",
       "0           1.0  \n",
       "1           1.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "235649      0.0  \n",
       "235650      0.0  \n",
       "235651      0.0  \n",
       "235652      0.0  \n",
       "235653      1.0  \n",
       "\n",
       "[235654 rows x 24 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo mô hình Logistic Regression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Huấn luyện mô hình trên tập train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7954256131715183\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7940252906730034\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7948739709751337\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7954680471866248\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7959685974962869\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.795374496074687\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7963505198387439\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7940165499681732\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7948652662847443\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.7959261616804583\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7952294513349374\n",
      "Standard Deviation of Accuracy: 0.0007473360709186283\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình XGBoost trên tập train\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7997963167274887\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8000084868030213\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8011542052108971\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8025120936943053\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8032675578187991\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8029705071079991\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8013579461065139\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8004243581582856\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8014852535539996\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8015276893698281\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8014504414551137\n",
      "Standard Deviation of Accuracy: 0.001127444727467978\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình LightGBM trên tập train\n",
    "        model = lgb.LGBMClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "[LightGBM] [Info] Number of positive: 168722, number of negative: 43366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795528 -> initscore=1.358577\n",
      "[LightGBM] [Info] Start training from score 1.358577\n",
      "Accuracy for fold 1: 0.8003903929389798\n",
      "Training on fold 2...\n",
      "[LightGBM] [Info] Number of positive: 168779, number of negative: 43309\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4125\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795797 -> initscore=1.360230\n",
      "[LightGBM] [Info] Start training from score 1.360230\n",
      "Accuracy for fold 2: 0.7999660527879148\n",
      "Training on fold 3...\n",
      "[LightGBM] [Info] Number of positive: 168740, number of negative: 43348\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795613 -> initscore=1.359099\n",
      "[LightGBM] [Info] Start training from score 1.359099\n",
      "Accuracy for fold 3: 0.800899601120258\n",
      "Training on fold 4...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43377\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795476 -> initscore=1.358258\n",
      "[LightGBM] [Info] Start training from score 1.358258\n",
      "Accuracy for fold 4: 0.8022150555885598\n",
      "Training on fold 5...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 5: 0.803182686187142\n",
      "Training on fold 6...\n",
      "[LightGBM] [Info] Number of positive: 168717, number of negative: 43372\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795501 -> initscore=1.358409\n",
      "[LightGBM] [Info] Start training from score 1.358409\n",
      "Accuracy for fold 6: 0.8018671758964566\n",
      "Training on fold 7...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 7: 0.8022915340547422\n",
      "Training on fold 8...\n",
      "[LightGBM] [Info] Number of positive: 168752, number of negative: 43337\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795666 -> initscore=1.359423\n",
      "[LightGBM] [Info] Start training from score 1.359423\n",
      "Accuracy for fold 8: 0.8015701251856567\n",
      "Training on fold 9...\n",
      "[LightGBM] [Info] Number of positive: 168742, number of negative: 43347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795619 -> initscore=1.359133\n",
      "[LightGBM] [Info] Start training from score 1.359133\n",
      "Accuracy for fold 9: 0.7997029492892\n",
      "Training on fold 10...\n",
      "[LightGBM] [Info] Number of positive: 168708, number of negative: 43381\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795459 -> initscore=1.358148\n",
      "[LightGBM] [Info] Start training from score 1.358148\n",
      "Accuracy for fold 10: 0.8020793549755995\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.801416492802451\n",
      "Standard Deviation of Accuracy: 0.0010746631452779184\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_lightgbm_kfold(processed_data, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
