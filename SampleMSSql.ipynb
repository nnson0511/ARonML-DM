{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '192.168.1.212'\n",
    "database = 'master'\n",
    "username = 'test'\n",
    "password = 'tester2024'\n",
    "\n",
    "mssql_conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        select *\n",
      "        from ProjectNew..FullCompanyInfo\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "msql_query = f\"\"\"\n",
    "        select *\n",
    "        from ProjectNew..FullCompanyInfo\n",
    "        \"\"\"\n",
    "print(msql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối cơ sở dữ liệu thành công\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nnson\\AppData\\Local\\Temp\\ipykernel_18816\\1879818091.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql_query(msql_query, mssql_conn)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mssql_conn = pyodbc.connect(mssql_conn_str)\n",
    "    print(\"Kết nối cơ sở dữ liệu thành công\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Lỗi khi kết nối cơ sở dữ liệu: {e}\")\n",
    "\n",
    "data = pd.read_sql_query(msql_query, mssql_conn)\n",
    "\n",
    "mssql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df1):\n",
    "    df = df1.copy()\n",
    "\n",
    "    # One-hot encoding cho cột CompanyType\n",
    "    df = pd.get_dummies(df, columns=['CompanyType'], prefix='Type')\n",
    "        \n",
    "    if 'CompanyId' in df.columns:\n",
    "        df.drop(columns=['CompanyId'], inplace=True)\n",
    "\n",
    "    # for id in range(11, 25):\n",
    "    #     mean_col_name = f'{id}_mean'\n",
    "        \n",
    "    #     # Tìm tất cả các cột tương ứng với id hiện tại\n",
    "    #     cols_to_avg = [f'{year}_{id}' for year in range(2015, 2022)]\n",
    "        \n",
    "    #     # Tính giá trị trung bình và thêm vào cột mới\n",
    "    #     df[mean_col_name] = df[cols_to_avg].mean(axis=1)\n",
    "\n",
    "    # # Xóa các cột cũ\n",
    "    # df.drop(columns=[f'{year}_{id}' for year in range(2015, 2022) for id in range(11, 25)], inplace=True)\n",
    "\n",
    "    # Normalize các cột còn lại với giá trị từ 0 đến 1\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    # Thay thế tất cả các giá trị NaN trong df thành 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processing_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyAge</th>\n",
       "      <th>FDI</th>\n",
       "      <th>CapitalAmount</th>\n",
       "      <th>NumberOfLabors</th>\n",
       "      <th>Region</th>\n",
       "      <th>FS11</th>\n",
       "      <th>FS12</th>\n",
       "      <th>FS13</th>\n",
       "      <th>FS14</th>\n",
       "      <th>FS15</th>\n",
       "      <th>...</th>\n",
       "      <th>FS20</th>\n",
       "      <th>FS21</th>\n",
       "      <th>FS22</th>\n",
       "      <th>FS23</th>\n",
       "      <th>FS24</th>\n",
       "      <th>Status</th>\n",
       "      <th>Type_LLC1</th>\n",
       "      <th>Type_LLC2</th>\n",
       "      <th>Type_PE</th>\n",
       "      <th>Type_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071832</td>\n",
       "      <td>0.077137</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135614</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071798</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071794</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135629</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071799</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235649</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235650</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235651</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066642</td>\n",
       "      <td>0.096372</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.061944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071841</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.182202</td>\n",
       "      <td>0.217624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235652</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066632</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071796</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235653</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235654 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CompanyAge  FDI  CapitalAmount  NumberOfLabors  Region      FS11  \\\n",
       "0         0.176471  0.0       0.000292        0.000061     1.0  0.066675   \n",
       "1         0.382353  0.0       0.000097        0.000076     0.0  0.066634   \n",
       "2         0.147059  0.0       0.000015        0.000046     1.0  0.066630   \n",
       "3         0.147059  0.0       0.000048        0.000015     1.0  0.066635   \n",
       "4         0.088235  0.0       0.000002        0.000153     1.0  0.066627   \n",
       "...            ...  ...            ...             ...     ...       ...   \n",
       "235649    0.235294  0.0       0.000049        0.000153     1.0  0.066638   \n",
       "235650    0.117647  0.0       0.000005        0.000015     1.0  0.066627   \n",
       "235651    0.205882  0.0       0.000243        0.000153     1.0  0.066642   \n",
       "235652    0.088235  0.0       0.000024        0.000015     1.0  0.066632   \n",
       "235653    0.088235  0.0       0.000046        0.000076     1.0  0.066626   \n",
       "\n",
       "            FS12      FS13      FS14      FS15  ...      FS20      FS21  \\\n",
       "0       0.000000  0.135581  0.023858  0.061939  ...  0.071832  0.077137   \n",
       "1       0.000000  0.135614  0.023858  0.061955  ...  0.071798  0.077136   \n",
       "2       0.000000  0.135581  0.023858  0.061939  ...  0.071794  0.077136   \n",
       "3       0.000000  0.135629  0.023858  0.061945  ...  0.071799  0.077144   \n",
       "4       0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "235649  0.000000  0.000000  0.000000  0.000000  ...  0.071801  0.000000   \n",
       "235650  0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "235651  0.096372  0.135581  0.023917  0.061944  ...  0.071841  0.077144   \n",
       "235652  0.096354  0.135581  0.023858  0.061939  ...  0.071796  0.077136   \n",
       "235653  0.000000  0.135581  0.023858  0.061939  ...  0.071791  0.077136   \n",
       "\n",
       "            FS22      FS23      FS24  Status  Type_LLC1  Type_LLC2  Type_PE  \\\n",
       "0       0.033333  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "1       0.033332  0.182198  0.217620     1.0        0.0        0.0      0.0   \n",
       "2       0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "3       0.033334  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "4       0.033332  0.182198  0.217620     0.0        1.0        0.0      0.0   \n",
       "...          ...       ...       ...     ...        ...        ...      ...   \n",
       "235649  0.000000  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235650  0.033332  0.182198  0.217620     1.0        0.0        1.0      0.0   \n",
       "235651  0.033339  0.182202  0.217624     1.0        1.0        0.0      0.0   \n",
       "235652  0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235653  0.033332  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "\n",
       "        Type_SC  \n",
       "0           1.0  \n",
       "1           1.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "235649      0.0  \n",
       "235650      0.0  \n",
       "235651      0.0  \n",
       "235652      0.0  \n",
       "235653      1.0  \n",
       "\n",
       "[235654 rows x 24 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo mô hình Logistic Regression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Huấn luyện mô hình trên tập train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7954256131715183\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7940252906730034\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7948739709751337\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7954680471866248\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7959685974962869\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.795374496074687\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7963505198387439\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7940165499681732\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7948652662847443\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.7959261616804583\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7952294513349374\n",
      "Standard Deviation of Accuracy: 0.0007473360709186283\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình XGBoost trên tập train\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7997963167274887\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8000084868030213\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8011542052108971\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8025120936943053\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8032675578187991\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8029705071079991\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8013579461065139\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8004243581582856\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8014852535539996\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8015276893698281\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8014504414551137\n",
      "Standard Deviation of Accuracy: 0.001127444727467978\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy mô hình XGBoost cho thấy độ chính xác (accuracy) tổng thể là 0.80, nhưng có sự chênh lệch lớn về hiệu suất giữa hai lớp. \n",
    "\n",
    "Status 0.0 có recall rất thấp (0.15), cho thấy mô hình gặp khó khăn trong việc dự đoán đúng các mẫu thuộc lớp này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def train_lightgbm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình LightGBM trên tập train\n",
    "        model = lgb.LGBMClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "[LightGBM] [Info] Number of positive: 168722, number of negative: 43366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795528 -> initscore=1.358577\n",
      "[LightGBM] [Info] Start training from score 1.358577\n",
      "Accuracy for fold 1: 0.8003903929389798\n",
      "Training on fold 2...\n",
      "[LightGBM] [Info] Number of positive: 168779, number of negative: 43309\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4125\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795797 -> initscore=1.360230\n",
      "[LightGBM] [Info] Start training from score 1.360230\n",
      "Accuracy for fold 2: 0.7999660527879148\n",
      "Training on fold 3...\n",
      "[LightGBM] [Info] Number of positive: 168740, number of negative: 43348\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795613 -> initscore=1.359099\n",
      "[LightGBM] [Info] Start training from score 1.359099\n",
      "Accuracy for fold 3: 0.800899601120258\n",
      "Training on fold 4...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43377\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795476 -> initscore=1.358258\n",
      "[LightGBM] [Info] Start training from score 1.358258\n",
      "Accuracy for fold 4: 0.8022150555885598\n",
      "Training on fold 5...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 5: 0.803182686187142\n",
      "Training on fold 6...\n",
      "[LightGBM] [Info] Number of positive: 168717, number of negative: 43372\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795501 -> initscore=1.358409\n",
      "[LightGBM] [Info] Start training from score 1.358409\n",
      "Accuracy for fold 6: 0.8018671758964566\n",
      "Training on fold 7...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 7: 0.8022915340547422\n",
      "Training on fold 8...\n",
      "[LightGBM] [Info] Number of positive: 168752, number of negative: 43337\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795666 -> initscore=1.359423\n",
      "[LightGBM] [Info] Start training from score 1.359423\n",
      "Accuracy for fold 8: 0.8015701251856567\n",
      "Training on fold 9...\n",
      "[LightGBM] [Info] Number of positive: 168742, number of negative: 43347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795619 -> initscore=1.359133\n",
      "[LightGBM] [Info] Start training from score 1.359133\n",
      "Accuracy for fold 9: 0.7997029492892\n",
      "Training on fold 10...\n",
      "[LightGBM] [Info] Number of positive: 168708, number of negative: 43381\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795459 -> initscore=1.358148\n",
      "[LightGBM] [Info] Start training from score 1.358148\n",
      "Accuracy for fold 10: 0.8020793549755995\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.801416492802451\n",
      "Standard Deviation of Accuracy: 0.0010746631452779184\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_lightgbm_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy mô hình LightGBM cho thấy độ chính xác (accuracy) tổng thể là 0.80, nhưng có sự chênh lệch lớn về hiệu suất giữa hai lớp. \n",
    "\n",
    "Status 0.0 có recall rất thấp (0.07), cho thấy mô hình gặp khó khăn trong việc dự đoán đúng các mẫu thuộc lớp này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def train_catboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "    \n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình CatBoost trên tập train\n",
    "        model = cb.CatBoostClassifier(verbose=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8006449970296189\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.800814733090045\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8011966392260036\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8037851141475006\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8046679397411415\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8034373010821133\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8018671758964566\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8028431996605134\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8030978145554848\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8027583280288564\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8025113242457735\n",
      "Standard Deviation of Accuracy: 0.0012708662352027091\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_catboost_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_hist_gradient_boosting_kfold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "def train_hist_gradient_boosting_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình HistGradientBoostingClassifier trên tập train\n",
    "        model = HistGradientBoostingClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_class_weight_kfold\n",
    "\n",
    "# Imbalanced Learning Techniques - Class Weight Adjustment: Điều chỉnh trọng số lớp để mô hình tập trung hơn vào lớp thiểu số.\n",
    "\n",
    "def train_random_forest_class_weight_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier với class_weight='balanced'\n",
    "        model = RandomForestClassifier(class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_grid_search\n",
    "\n",
    "# Hyperparameter Tuning - Grid Search\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "def train_random_forest_grid_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Grid Search với RandomForestClassifier\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Huấn luyện Grid Search trên toàn bộ dữ liệu\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Grid Search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_random_search\n",
    "\n",
    "# Hyperparameter Tuning - Random Search\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "\n",
    "def train_random_forest_random_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Random Search\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5)\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Random Search với RandomForestClassifier\n",
    "    random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, n_iter=100, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "    # Huấn luyện Random Search trên toàn bộ dữ liệu\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Random Search\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", random_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", random_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))\n",
    "\n",
    "# Giả sử processed_data là DataFrame của bạn\n",
    "# processed_data = pd.read_csv('your_processed_data.csv')\n",
    "\n",
    "# Gọi hàm với số lần fold là 5\n",
    "# train_random_forest_random_search(processed_data, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stacking_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_stacking_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo các mô hình cơ bản\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('gb', GradientBoostingClassifier())\n",
    "        ]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình StackingClassifier trên tập train\n",
    "        model = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LogisticRegression()\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n",
    "# Giả sử processed_data là DataFrame của bạn\n",
    "# processed_data = pd.read_csv('your_processed_data.csv')\n",
    "\n",
    "# Gọi hàm với số lần fold là 5\n",
    "# train_stacking_kfold(processed_data, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_deep_learning_kfold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_deep_learning_kfold(processed_data, k=5, epochs=50, batch_size=32):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Chuẩn hóa dữ liệu\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Xây dựng mô hình\n",
    "        model = build_model(X_train.shape[1])\n",
    "        \n",
    "        # Đào tạo mô hình với tpdm (ProgbarLogger)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, \n",
    "                  validation_data=(X_test, y_test), callbacks=[ProgbarLogger()])\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_automl_kfold with tpot\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "def train_automl_kfold(processed_data, k=5, generations=50, population_size=50):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình TPOTClassifier trên tập train\n",
    "        model = TPOTClassifier(generations=generations, population_size=population_size, verbosity=2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train_smote_kfold\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_smote_kfold\u001b[39m(processed_data, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m processed_data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditedNearestNeighbours\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_adasyn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ADASYN\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_random_over_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADASYN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomOverSampler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeansSMOTE\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVMSMOTE, BorderlineSMOTE\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVMSMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\cluster.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MiniBatchKMeans\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\cluster\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.cluster` module gathers popular unsupervised clustering\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03malgorithms.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_affinity_propagation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AffinityPropagation, affinity_propagation\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_agglomerative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     AgglomerativeClustering,\n\u001b[0;32m      9\u001b[0m     FeatureAgglomeration,\n\u001b[0;32m     10\u001b[0m     linkage_tree,\n\u001b[0;32m     11\u001b[0m     ward_tree,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpectralBiclustering, SpectralCoclustering\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_birch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Birch\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast'\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _hierarchical_fast \u001b[38;5;28;01mas\u001b[39;00m _hierarchical  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_feature_agglomeration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgglomerationTransform\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# For non fully-connected graphs\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fix_connectivity\u001b[39m(X, connectivity, affinity):\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\cluster\\_feature_agglomeration.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_Xt_in_inverse_transform\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Mixin class for feature agglomeration.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py)"
     ]
    }
   ],
   "source": [
    "# train_smote_kfold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train đã được resample\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7995841466519562\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8006025630145124\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.800899601120258\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8022150555885598\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8030129429238277\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8019944833439423\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8028007638446849\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.801060895395714\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7998302567366857\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8017398684489709\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8013740577069111\n",
      "Standard Deviation of Accuracy: 0.0011159276334650715\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_hist_gradient_boosting_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7945769328693881\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7942798947636426\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7941950267334296\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7950012730204532\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7957564184171441\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.7966051347337153\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7957139826013155\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7949501379164015\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7963080840229153\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.7952896244430299\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7952676509521435\n",
      "Standard Deviation of Accuracy: 0.0007800641290058936\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7674615972163286\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7678435033522872\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.762496817448867\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7661037087329203\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7672395501803522\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.7691915977084659\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7662635264162954\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7648631444939529\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.766390833863781\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.768512624655209\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7666366904068459\n",
      "Standard Deviation of Accuracy: 0.001825233375266761\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_class_weight_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib.externals.loky.backend.popen_loky_win32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_random_forest_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[126], line 31\u001b[0m, in \u001b[0;36mtrain_random_forest_grid_search\u001b[1;34m(processed_data, k)\u001b[0m\n\u001b[0;32m     28\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mRandomForestClassifier(), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mkf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Huấn luyện Grid Search trên toàn bộ dữ liệu\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Lấy mô hình tốt nhất từ Grid Search\u001b[39;00m\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    966\u001b[0m     return results\n\u001b[0;32m    968\u001b[0m self._run_search(evaluate_candidates)\n\u001b[1;32m--> 970\u001b[0m # multimetric is determined here because in the case of a callable\n\u001b[0;32m    971\u001b[0m # self.scoring the return type is only known after calling\n\u001b[0;32m    972\u001b[0m first_test_score = all_out[0][\"test_scores\"]\n\u001b[0;32m    973\u001b[0m self.multimetric_ = isinstance(first_test_score, dict)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1507\u001b[0m _required_parameters = [\"estimator\", \"param_grid\"]\n\u001b[0;32m   1509\u001b[0m _parameter_constraints: dict = {\n\u001b[0;32m   1510\u001b[0m     **BaseSearchCV._parameter_constraints,\n\u001b[0;32m   1511\u001b[0m     \"param_grid\": [dict, list],\n\u001b[0;32m   1512\u001b[0m }\n\u001b[0;32m   1514\u001b[0m def __init__(\n\u001b[0;32m   1515\u001b[0m     self,\n\u001b[0;32m   1516\u001b[0m     estimator,\n\u001b[0;32m   1517\u001b[0m     param_grid,\n\u001b[0;32m   1518\u001b[0m     *,\n\u001b[0;32m   1519\u001b[0m     scoring=None,\n\u001b[0;32m   1520\u001b[0m     n_jobs=None,\n\u001b[0;32m   1521\u001b[0m     refit=True,\n\u001b[0;32m   1522\u001b[0m     cv=None,\n\u001b[0;32m   1523\u001b[0m     verbose=0,\n\u001b[0;32m   1524\u001b[0m     pre_dispatch=\"2*n_jobs\",\n\u001b[0;32m   1525\u001b[0m     error_score=np.nan,\n\u001b[0;32m   1526\u001b[0m     return_train_score=False,\n\u001b[1;32m-> 1527\u001b[0m ):\n\u001b[0;32m   1528\u001b[0m     super().__init__(\n\u001b[0;32m   1529\u001b[0m         estimator=estimator,\n\u001b[0;32m   1530\u001b[0m         scoring=scoring,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         return_train_score=return_train_score,\n\u001b[0;32m   1538\u001b[0m     )\n\u001b[0;32m   1539\u001b[0m     self.param_grid = param_grid\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[0;32m    914\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    915\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m--> 916\u001b[0m         clone(base_estimator),\n\u001b[0;32m    917\u001b[0m         X,\n\u001b[0;32m    918\u001b[0m         y,\n\u001b[0;32m    919\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    920\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    921\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    922\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    923\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    925\u001b[0m     )\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    928\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    929\u001b[0m     )\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1950\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1588\u001b[0m, in \u001b[0;36m_get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1571\u001b[0m, in \u001b[0;36m_start\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1462\u001b[0m, in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1384\u001b[0m, in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:600\u001b[0m, in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py:225\u001b[0m, in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1248\u001b[0m, in \u001b[0;36msubmit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1220\u001b[0m, in \u001b[0;36m_ensure_executor_running\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1209\u001b[0m, in \u001b[0;36m_adjust_process_count\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\process.py:42\u001b[0m, in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'joblib.externals.loky.backend.popen_loky_win32'"
     ]
    }
   ],
   "source": [
    "train_random_forest_grid_search(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_random_search(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8011542052108971\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7992446745311041\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7997114486972757\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8003055249087668\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8009335879482283\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8014003819223424\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8009760237640569\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8025885847655421\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8000424358158286\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8011882028431997\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8007545070407242\n",
      "Standard Deviation of Accuracy: 0.000910284629604158\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_stacking_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 700/6628\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 801us/step - accuracy: 0.7935 - loss: 0.5103"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_deep_learning_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[136], line 43\u001b[0m, in \u001b[0;36mtrain_deep_learning_kfold\u001b[1;34m(processed_data, k, epochs, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Đào tạo mô hình với tpdm (ProgbarLogger)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mProgbarLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Dự đoán trên tập test\u001b[39;00m\n\u001b[0;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\nnson\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_deep_learning_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_automl_kfold(processed_data, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
