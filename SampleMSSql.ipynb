{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '192.168.1.212'\n",
    "database = 'master'\n",
    "username = 'test'\n",
    "password = 'tester2024'\n",
    "\n",
    "mssql_conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        select *\n",
      "        from ProjectNew..FullCompanyInfo\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "msql_query = f\"\"\"\n",
    "        select *\n",
    "        from ProjectNew..FullCompanyInfo\n",
    "        \"\"\"\n",
    "print(msql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối cơ sở dữ liệu thành công\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8352\\1879818091.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql_query(msql_query, mssql_conn)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mssql_conn = pyodbc.connect(mssql_conn_str)\n",
    "    print(\"Kết nối cơ sở dữ liệu thành công\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Lỗi khi kết nối cơ sở dữ liệu: {e}\")\n",
    "\n",
    "data = pd.read_sql_query(msql_query, mssql_conn)\n",
    "\n",
    "mssql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df1):\n",
    "    df = df1.copy()\n",
    "\n",
    "    # One-hot encoding cho cột CompanyType\n",
    "    df = pd.get_dummies(df, columns=['CompanyType'], prefix='Type')\n",
    "        \n",
    "    if 'CompanyId' in df.columns:\n",
    "        df.drop(columns=['CompanyId'], inplace=True)\n",
    "\n",
    "    # for id in range(11, 25):\n",
    "    #     mean_col_name = f'{id}_mean'\n",
    "        \n",
    "    #     # Tìm tất cả các cột tương ứng với id hiện tại\n",
    "    #     cols_to_avg = [f'{year}_{id}' for year in range(2015, 2022)]\n",
    "        \n",
    "    #     # Tính giá trị trung bình và thêm vào cột mới\n",
    "    #     df[mean_col_name] = df[cols_to_avg].mean(axis=1)\n",
    "\n",
    "    # # Xóa các cột cũ\n",
    "    # df.drop(columns=[f'{year}_{id}' for year in range(2015, 2022) for id in range(11, 25)], inplace=True)\n",
    "\n",
    "    # Normalize các cột còn lại với giá trị từ 0 đến 1\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    # Thay thế tất cả các giá trị NaN trong df thành 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processing_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyAge</th>\n",
       "      <th>FDI</th>\n",
       "      <th>CapitalAmount</th>\n",
       "      <th>NumberOfLabors</th>\n",
       "      <th>Region</th>\n",
       "      <th>FS11</th>\n",
       "      <th>FS12</th>\n",
       "      <th>FS13</th>\n",
       "      <th>FS14</th>\n",
       "      <th>FS15</th>\n",
       "      <th>...</th>\n",
       "      <th>FS20</th>\n",
       "      <th>FS21</th>\n",
       "      <th>FS22</th>\n",
       "      <th>FS23</th>\n",
       "      <th>FS24</th>\n",
       "      <th>Status</th>\n",
       "      <th>Type_LLC1</th>\n",
       "      <th>Type_LLC2</th>\n",
       "      <th>Type_PE</th>\n",
       "      <th>Type_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071832</td>\n",
       "      <td>0.077137</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135614</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071798</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071794</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135629</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071799</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235649</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235650</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235651</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066642</td>\n",
       "      <td>0.096372</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.061944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071841</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.182202</td>\n",
       "      <td>0.217624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235652</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066632</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071796</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235653</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.061939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235654 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CompanyAge  FDI  CapitalAmount  NumberOfLabors  Region      FS11  \\\n",
       "0         0.176471  0.0       0.000292        0.000061     1.0  0.066675   \n",
       "1         0.382353  0.0       0.000097        0.000076     0.0  0.066634   \n",
       "2         0.147059  0.0       0.000015        0.000046     1.0  0.066630   \n",
       "3         0.147059  0.0       0.000048        0.000015     1.0  0.066635   \n",
       "4         0.088235  0.0       0.000002        0.000153     1.0  0.066627   \n",
       "...            ...  ...            ...             ...     ...       ...   \n",
       "235649    0.235294  0.0       0.000049        0.000153     1.0  0.066638   \n",
       "235650    0.117647  0.0       0.000005        0.000015     1.0  0.066627   \n",
       "235651    0.205882  0.0       0.000243        0.000153     1.0  0.066642   \n",
       "235652    0.088235  0.0       0.000024        0.000015     1.0  0.066632   \n",
       "235653    0.088235  0.0       0.000046        0.000076     1.0  0.066626   \n",
       "\n",
       "            FS12      FS13      FS14      FS15  ...      FS20      FS21  \\\n",
       "0       0.000000  0.135581  0.023858  0.061939  ...  0.071832  0.077137   \n",
       "1       0.000000  0.135614  0.023858  0.061955  ...  0.071798  0.077136   \n",
       "2       0.000000  0.135581  0.023858  0.061939  ...  0.071794  0.077136   \n",
       "3       0.000000  0.135629  0.023858  0.061945  ...  0.071799  0.077144   \n",
       "4       0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "235649  0.000000  0.000000  0.000000  0.000000  ...  0.071801  0.000000   \n",
       "235650  0.000000  0.135581  0.023858  0.061939  ...  0.071792  0.077136   \n",
       "235651  0.096372  0.135581  0.023917  0.061944  ...  0.071841  0.077144   \n",
       "235652  0.096354  0.135581  0.023858  0.061939  ...  0.071796  0.077136   \n",
       "235653  0.000000  0.135581  0.023858  0.061939  ...  0.071791  0.077136   \n",
       "\n",
       "            FS22      FS23      FS24  Status  Type_LLC1  Type_LLC2  Type_PE  \\\n",
       "0       0.033333  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "1       0.033332  0.182198  0.217620     1.0        0.0        0.0      0.0   \n",
       "2       0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "3       0.033334  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "4       0.033332  0.182198  0.217620     0.0        1.0        0.0      0.0   \n",
       "...          ...       ...       ...     ...        ...        ...      ...   \n",
       "235649  0.000000  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235650  0.033332  0.182198  0.217620     1.0        0.0        1.0      0.0   \n",
       "235651  0.033339  0.182202  0.217624     1.0        1.0        0.0      0.0   \n",
       "235652  0.033332  0.182198  0.217620     1.0        1.0        0.0      0.0   \n",
       "235653  0.033332  0.182198  0.217620     0.0        0.0        0.0      0.0   \n",
       "\n",
       "        Type_SC  \n",
       "0           1.0  \n",
       "1           1.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "235649      0.0  \n",
       "235650      0.0  \n",
       "235651      0.0  \n",
       "235652      0.0  \n",
       "235653      1.0  \n",
       "\n",
       "[235654 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo mô hình Logistic Regression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Huấn luyện mô hình trên tập train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7954256131715183\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7940252906730034\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7948739709751337\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7954680471866248\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7959685974962869\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.795374496074687\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7963505198387439\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7940165499681732\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7948652662847443\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.7959261616804583\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7952294513349374\n",
      "Standard Deviation of Accuracy: 0.0007473360709186283\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.00      0.01      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.59      0.50      0.45     23565\n",
      "weighted avg       0.71      0.80      0.71     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình XGBoost trên tập train\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7997963167274887\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8000084868030213\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8011542052108971\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8025120936943053\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8032675578187991\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8029705071079991\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8013579461065139\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8004243581582856\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8014852535539996\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8015276893698281\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8014504414551137\n",
      "Standard Deviation of Accuracy: 0.001127444727467978\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.70      0.53      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy mô hình XGBoost cho thấy độ chính xác (accuracy) tổng thể là 0.80, nhưng có sự chênh lệch lớn về hiệu suất giữa hai lớp. \n",
    "\n",
    "Status 0.0 có recall rất thấp (0.15), cho thấy mô hình gặp khó khăn trong việc dự đoán đúng các mẫu thuộc lớp này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def train_lightgbm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình LightGBM trên tập train\n",
    "        model = lgb.LGBMClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "[LightGBM] [Info] Number of positive: 168722, number of negative: 43366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795528 -> initscore=1.358577\n",
      "[LightGBM] [Info] Start training from score 1.358577\n",
      "Accuracy for fold 1: 0.8003903929389798\n",
      "Training on fold 2...\n",
      "[LightGBM] [Info] Number of positive: 168779, number of negative: 43309\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4125\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795797 -> initscore=1.360230\n",
      "[LightGBM] [Info] Start training from score 1.360230\n",
      "Accuracy for fold 2: 0.7999660527879148\n",
      "Training on fold 3...\n",
      "[LightGBM] [Info] Number of positive: 168740, number of negative: 43348\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795613 -> initscore=1.359099\n",
      "[LightGBM] [Info] Start training from score 1.359099\n",
      "Accuracy for fold 3: 0.800899601120258\n",
      "Training on fold 4...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43377\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212088, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795476 -> initscore=1.358258\n",
      "[LightGBM] [Info] Start training from score 1.358258\n",
      "Accuracy for fold 4: 0.8022150555885598\n",
      "Training on fold 5...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 5: 0.803182686187142\n",
      "Training on fold 6...\n",
      "[LightGBM] [Info] Number of positive: 168717, number of negative: 43372\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795501 -> initscore=1.358409\n",
      "[LightGBM] [Info] Start training from score 1.358409\n",
      "Accuracy for fold 6: 0.8018671758964566\n",
      "Training on fold 7...\n",
      "[LightGBM] [Info] Number of positive: 168711, number of negative: 43378\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795473 -> initscore=1.358235\n",
      "[LightGBM] [Info] Start training from score 1.358235\n",
      "Accuracy for fold 7: 0.8022915340547422\n",
      "Training on fold 8...\n",
      "[LightGBM] [Info] Number of positive: 168752, number of negative: 43337\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795666 -> initscore=1.359423\n",
      "[LightGBM] [Info] Start training from score 1.359423\n",
      "Accuracy for fold 8: 0.8015701251856567\n",
      "Training on fold 9...\n",
      "[LightGBM] [Info] Number of positive: 168742, number of negative: 43347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795619 -> initscore=1.359133\n",
      "[LightGBM] [Info] Start training from score 1.359133\n",
      "Accuracy for fold 9: 0.7997029492892\n",
      "Training on fold 10...\n",
      "[LightGBM] [Info] Number of positive: 168708, number of negative: 43381\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 212089, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.795459 -> initscore=1.358148\n",
      "[LightGBM] [Info] Start training from score 1.358148\n",
      "Accuracy for fold 10: 0.8020793549755995\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.801416492802451\n",
      "Standard Deviation of Accuracy: 0.0010746631452779184\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.72      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_lightgbm_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy mô hình LightGBM cho thấy độ chính xác (accuracy) tổng thể là 0.80, nhưng có sự chênh lệch lớn về hiệu suất giữa hai lớp. \n",
    "\n",
    "Status 0.0 có recall rất thấp (0.07), cho thấy mô hình gặp khó khăn trong việc dự đoán đúng các mẫu thuộc lớp này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "def train_catboost_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "    \n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình CatBoost trên tập train\n",
    "        model = cb.CatBoostClassifier(verbose=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8006449970296189\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.800814733090045\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.8011966392260036\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8037851141475006\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8046679397411415\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8034373010821133\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8018671758964566\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8028431996605134\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8030978145554848\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8027583280288564\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8025113242457735\n",
      "Standard Deviation of Accuracy: 0.0012708662352027091\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.09      0.15      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.54      0.52     23565\n",
      "weighted avg       0.77      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_catboost_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_hist_gradient_boosting_kfold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "def train_hist_gradient_boosting_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình HistGradientBoostingClassifier trên tập train\n",
    "        model = HistGradientBoostingClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_class_weight_kfold\n",
    "\n",
    "# Imbalanced Learning Techniques - Class Weight Adjustment: Điều chỉnh trọng số lớp để mô hình tập trung hơn vào lớp thiểu số.\n",
    "\n",
    "def train_random_forest_class_weight_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier với class_weight='balanced'\n",
    "        model = RandomForestClassifier(class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_grid_search\n",
    "\n",
    "# Hyperparameter Tuning - Grid Search\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "def train_random_forest_grid_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Grid Search với RandomForestClassifier\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Huấn luyện Grid Search trên toàn bộ dữ liệu\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Grid Search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_random_forest_random_search\n",
    "\n",
    "# Hyperparameter Tuning - Random Search\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "def train_random_forest_random_search(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Thiết lập các tham số cho Random Search\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5)\n",
    "    }\n",
    "\n",
    "    # Khởi tạo Random Search với RandomForestClassifier\n",
    "    random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, n_iter=100, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "    # Huấn luyện Random Search trên toàn bộ dữ liệu\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Lấy mô hình tốt nhất từ Random Search\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # In ra các tham số tốt nhất\n",
    "    print(\"Best parameters found: \", random_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy: \", random_search.best_score_)\n",
    "\n",
    "    # Dự đoán và đánh giá mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"Classification Report for best model:\")\n",
    "    print(classification_report(y, y_pred, zero_division=0))\n",
    "\n",
    "# Giả sử processed_data là DataFrame của bạn\n",
    "# processed_data = pd.read_csv('your_processed_data.csv')\n",
    "\n",
    "# Gọi hàm với số lần fold là 5\n",
    "# train_random_forest_random_search(processed_data, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stacking_kfold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_stacking_kfold(processed_data, k=5):\n",
    "    # Sao chép DataFrame và bỏ qua cảnh báo UndefinedMetricWarning\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    # Chia dữ liệu thành biến độc lập (X) và biến phụ thuộc (y)\n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    # Khởi tạo KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo các mô hình cơ bản\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('gb', GradientBoostingClassifier())\n",
    "        ]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình StackingClassifier trên tập train\n",
    "        model = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LogisticRegression()\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n",
    "# Giả sử processed_data là DataFrame của bạn\n",
    "# processed_data = pd.read_csv('your_processed_data.csv')\n",
    "\n",
    "# Gọi hàm với số lần fold là 5\n",
    "# train_stacking_kfold(processed_data, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_deep_learning_kfold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_deep_learning_kfold(processed_data, k=5, epochs=50, batch_size=32):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "        # Xây dựng mô hình\n",
    "        model = build_model(X_train.shape[1])\n",
    "        \n",
    "        # Đào tạo mô hình với tpdm (ProgbarLogger)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, \n",
    "                  validation_data=(X_test, y_test), callbacks=[ProgbarLogger()])\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_automl_kfold with tpot\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "def train_automl_kfold(processed_data, k=5, generations=50, population_size=50):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình TPOTClassifier trên tập train\n",
    "        model = TPOTClassifier(generations=generations, population_size=population_size, verbosity=2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smote_kfold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_smote_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE cho tập train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình RandomForestClassifier trên tập train đã được resample\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    # Tính toán và in ra báo cáo cuối cùng\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    # In ra classification report cho từng fold\n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7995841466519562\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.8006025630145124\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.800899601120258\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8022150555885598\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8030129429238277\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8019944833439423\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8028007638446849\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.801060895395714\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7998302567366857\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8017398684489709\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8013740577069111\n",
      "Standard Deviation of Accuracy: 0.0011159276334650715\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.07      0.12      4796\n",
      "         1.0       0.81      0.99      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.71      0.53      0.50     23565\n",
      "weighted avg       0.77      0.80      0.73     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_hist_gradient_boosting_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7945769328693881\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7942798947636426\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7941950267334296\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7950012730204532\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7957564184171441\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.7966051347337153\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7957139826013155\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7949501379164015\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.7963080840229153\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.7952896244430299\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7952676509521435\n",
      "Standard Deviation of Accuracy: 0.0007800641290058936\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      4796\n",
      "         1.0       0.82      0.96      0.88     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.65      0.55      0.55     23565\n",
      "weighted avg       0.75      0.80      0.75     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.7674615972163286\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7678435033522872\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.762496817448867\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.7661037087329203\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.7672395501803522\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.7691915977084659\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.7662635264162954\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.7648631444939529\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.766390833863781\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.768512624655209\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.7666366904068459\n",
      "Standard Deviation of Accuracy: 0.001825233375266761\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.29      0.34      4796\n",
      "         1.0       0.83      0.89      0.86     18769\n",
      "\n",
      "    accuracy                           0.77     23565\n",
      "   macro avg       0.62      0.59      0.60     23565\n",
      "weighted avg       0.74      0.77      0.75     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_class_weight_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n",
      "Best parameters found:  {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best cross-validation accuracy:  0.8023924971187096\n",
      "Classification Report for best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.15      0.26     48177\n",
      "         1.0       0.82      1.00      0.90    187477\n",
      "\n",
      "    accuracy                           0.82    235654\n",
      "   macro avg       0.87      0.58      0.58    235654\n",
      "weighted avg       0.84      0.82      0.77    235654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_grid_search(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Best parameters found:  {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 165}\n",
      "Best cross-validation accuracy:  0.8025410248150482\n",
      "Classification Report for best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.18      0.31     48177\n",
      "         1.0       0.83      1.00      0.90    187477\n",
      "\n",
      "    accuracy                           0.83    235654\n",
      "   macro avg       0.88      0.59      0.60    235654\n",
      "weighted avg       0.85      0.83      0.78    235654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_random_search(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Accuracy for fold 1: 0.8011542052108971\n",
      "Training on fold 2...\n",
      "Accuracy for fold 2: 0.7992446745311041\n",
      "Training on fold 3...\n",
      "Accuracy for fold 3: 0.7997114486972757\n",
      "Training on fold 4...\n",
      "Accuracy for fold 4: 0.8003055249087668\n",
      "Training on fold 5...\n",
      "Accuracy for fold 5: 0.8009335879482283\n",
      "Training on fold 6...\n",
      "Accuracy for fold 6: 0.8014003819223424\n",
      "Training on fold 7...\n",
      "Accuracy for fold 7: 0.8009760237640569\n",
      "Training on fold 8...\n",
      "Accuracy for fold 8: 0.8025885847655421\n",
      "Training on fold 9...\n",
      "Accuracy for fold 9: 0.8000424358158286\n",
      "Training on fold 10...\n",
      "Accuracy for fold 10: 0.8011882028431997\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.8007545070407242\n",
      "Standard Deviation of Accuracy: 0.000910284629604158\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.09      0.16      4796\n",
      "         1.0       0.81      0.98      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.69      0.54      0.52     23565\n",
      "weighted avg       0.76      0.80      0.74     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_stacking_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 807us/step - accuracy: 0.7963 - loss: 0.4978 - val_accuracy: 0.7958 - val_loss: 0.4918\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7946 - loss: 0.4936 - val_accuracy: 0.7960 - val_loss: 0.4901\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 713us/step - accuracy: 0.7952 - loss: 0.4914 - val_accuracy: 0.7963 - val_loss: 0.4959\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7954 - loss: 0.4911 - val_accuracy: 0.7967 - val_loss: 0.4890\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 707us/step - accuracy: 0.7954 - loss: 0.4910 - val_accuracy: 0.7967 - val_loss: 0.4885\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7959 - loss: 0.4889 - val_accuracy: 0.7966 - val_loss: 0.4886\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7959 - loss: 0.4891 - val_accuracy: 0.7967 - val_loss: 0.4886\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7961 - loss: 0.4888 - val_accuracy: 0.7967 - val_loss: 0.4880\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7969 - loss: 0.4883 - val_accuracy: 0.7961 - val_loss: 0.4885\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7949 - loss: 0.4906 - val_accuracy: 0.7965 - val_loss: 0.4888\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 706us/step - accuracy: 0.7958 - loss: 0.4897 - val_accuracy: 0.7966 - val_loss: 0.4878\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 692us/step - accuracy: 0.7957 - loss: 0.4890 - val_accuracy: 0.7968 - val_loss: 0.4878\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7962 - loss: 0.4881 - val_accuracy: 0.7967 - val_loss: 0.4883\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 686us/step - accuracy: 0.7962 - loss: 0.4887 - val_accuracy: 0.7964 - val_loss: 0.4885\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 701us/step - accuracy: 0.7963 - loss: 0.4885 - val_accuracy: 0.7965 - val_loss: 0.4889\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 710us/step - accuracy: 0.7965 - loss: 0.4878 - val_accuracy: 0.7965 - val_loss: 0.4880\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 707us/step - accuracy: 0.7960 - loss: 0.4889 - val_accuracy: 0.7967 - val_loss: 0.4889\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 677us/step - accuracy: 0.7946 - loss: 0.4909 - val_accuracy: 0.7965 - val_loss: 0.4886\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675us/step - accuracy: 0.7954 - loss: 0.4901 - val_accuracy: 0.7963 - val_loss: 0.4882\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675us/step - accuracy: 0.7938 - loss: 0.4910 - val_accuracy: 0.7964 - val_loss: 0.4883\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 681us/step - accuracy: 0.7959 - loss: 0.4887 - val_accuracy: 0.7962 - val_loss: 0.4877\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7949 - loss: 0.4899 - val_accuracy: 0.7965 - val_loss: 0.4878\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7957 - loss: 0.4892 - val_accuracy: 0.7964 - val_loss: 0.4874\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7963 - loss: 0.4882 - val_accuracy: 0.7966 - val_loss: 0.4873\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7951 - loss: 0.4901 - val_accuracy: 0.7964 - val_loss: 0.4883\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7954 - loss: 0.4890 - val_accuracy: 0.7964 - val_loss: 0.4875\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 697us/step - accuracy: 0.7972 - loss: 0.4867 - val_accuracy: 0.7966 - val_loss: 0.4879\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7954 - loss: 0.4891 - val_accuracy: 0.7967 - val_loss: 0.4878\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 699us/step - accuracy: 0.7953 - loss: 0.4894 - val_accuracy: 0.7967 - val_loss: 0.4874\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 697us/step - accuracy: 0.7963 - loss: 0.4884 - val_accuracy: 0.7965 - val_loss: 0.4874\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 687us/step - accuracy: 0.7975 - loss: 0.4863 - val_accuracy: 0.7964 - val_loss: 0.4885\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 673us/step - accuracy: 0.7962 - loss: 0.4881 - val_accuracy: 0.7964 - val_loss: 0.4868\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 687us/step - accuracy: 0.7969 - loss: 0.4879 - val_accuracy: 0.7965 - val_loss: 0.4875\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 676us/step - accuracy: 0.7967 - loss: 0.4874 - val_accuracy: 0.7964 - val_loss: 0.4877\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 673us/step - accuracy: 0.7950 - loss: 0.4889 - val_accuracy: 0.7964 - val_loss: 0.4886\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 673us/step - accuracy: 0.7956 - loss: 0.4878 - val_accuracy: 0.7965 - val_loss: 0.4866\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 669us/step - accuracy: 0.7943 - loss: 0.4905 - val_accuracy: 0.7962 - val_loss: 0.4877\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 674us/step - accuracy: 0.7963 - loss: 0.4875 - val_accuracy: 0.7962 - val_loss: 0.4874\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 684us/step - accuracy: 0.7975 - loss: 0.4866 - val_accuracy: 0.7964 - val_loss: 0.4872\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 679us/step - accuracy: 0.7954 - loss: 0.4886 - val_accuracy: 0.7962 - val_loss: 0.4875\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 675us/step - accuracy: 0.7969 - loss: 0.4871 - val_accuracy: 0.7964 - val_loss: 0.4873\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 678us/step - accuracy: 0.7970 - loss: 0.4861 - val_accuracy: 0.7964 - val_loss: 0.4869\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 682us/step - accuracy: 0.7959 - loss: 0.4881 - val_accuracy: 0.7964 - val_loss: 0.4873\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 671us/step - accuracy: 0.7953 - loss: 0.4887 - val_accuracy: 0.7964 - val_loss: 0.4867\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 681us/step - accuracy: 0.7970 - loss: 0.4864 - val_accuracy: 0.7962 - val_loss: 0.4877\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 689us/step - accuracy: 0.7964 - loss: 0.4870 - val_accuracy: 0.7964 - val_loss: 0.4868\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 686us/step - accuracy: 0.7950 - loss: 0.4895 - val_accuracy: 0.7961 - val_loss: 0.4868\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675us/step - accuracy: 0.7971 - loss: 0.4861 - val_accuracy: 0.7964 - val_loss: 0.4870\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 681us/step - accuracy: 0.7965 - loss: 0.4867 - val_accuracy: 0.7967 - val_loss: 0.4870\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 668us/step - accuracy: 0.7950 - loss: 0.4878 - val_accuracy: 0.7965 - val_loss: 0.4869\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step\n",
      "Accuracy for fold 1: 0.7964864635491811\n",
      "Training on fold 2...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 694us/step - accuracy: 0.7960 - loss: 0.4977 - val_accuracy: 0.7934 - val_loss: 0.4944\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 687us/step - accuracy: 0.7968 - loss: 0.4910 - val_accuracy: 0.7933 - val_loss: 0.4934\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 684us/step - accuracy: 0.7934 - loss: 0.4941 - val_accuracy: 0.7937 - val_loss: 0.4922\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7954 - loss: 0.4912 - val_accuracy: 0.7938 - val_loss: 0.4924\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7953 - loss: 0.4905 - val_accuracy: 0.7939 - val_loss: 0.4949\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 702us/step - accuracy: 0.7951 - loss: 0.4907 - val_accuracy: 0.7937 - val_loss: 0.4919\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 702us/step - accuracy: 0.7974 - loss: 0.4873 - val_accuracy: 0.7937 - val_loss: 0.4918\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 703us/step - accuracy: 0.7960 - loss: 0.4884 - val_accuracy: 0.7937 - val_loss: 0.4914\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 704us/step - accuracy: 0.7974 - loss: 0.4875 - val_accuracy: 0.7938 - val_loss: 0.4921\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7965 - loss: 0.4882 - val_accuracy: 0.7938 - val_loss: 0.4925\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 703us/step - accuracy: 0.7966 - loss: 0.4880 - val_accuracy: 0.7938 - val_loss: 0.4916\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7950 - loss: 0.4901 - val_accuracy: 0.7940 - val_loss: 0.4915\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7971 - loss: 0.4879 - val_accuracy: 0.7943 - val_loss: 0.4918\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7949 - loss: 0.4907 - val_accuracy: 0.7939 - val_loss: 0.4918\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7960 - loss: 0.4883 - val_accuracy: 0.7941 - val_loss: 0.4912\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7944 - loss: 0.4908 - val_accuracy: 0.7941 - val_loss: 0.4915\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7958 - loss: 0.4889 - val_accuracy: 0.7937 - val_loss: 0.4916\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7960 - loss: 0.4885 - val_accuracy: 0.7939 - val_loss: 0.4917\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7952 - loss: 0.4895 - val_accuracy: 0.7940 - val_loss: 0.4915\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7963 - loss: 0.4881 - val_accuracy: 0.7939 - val_loss: 0.4916\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7956 - loss: 0.4888 - val_accuracy: 0.7938 - val_loss: 0.4910\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7951 - loss: 0.4893 - val_accuracy: 0.7942 - val_loss: 0.4915\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7971 - loss: 0.4868 - val_accuracy: 0.7939 - val_loss: 0.4911\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7962 - loss: 0.4876 - val_accuracy: 0.7939 - val_loss: 0.4913\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 710us/step - accuracy: 0.7961 - loss: 0.4879 - val_accuracy: 0.7939 - val_loss: 0.4908\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7967 - loss: 0.4871 - val_accuracy: 0.7938 - val_loss: 0.4909\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 711us/step - accuracy: 0.7960 - loss: 0.4883 - val_accuracy: 0.7936 - val_loss: 0.4908\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7957 - loss: 0.4887 - val_accuracy: 0.7935 - val_loss: 0.4917\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7956 - loss: 0.4893 - val_accuracy: 0.7938 - val_loss: 0.4911\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7966 - loss: 0.4874 - val_accuracy: 0.7940 - val_loss: 0.4914\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7945 - loss: 0.4907 - val_accuracy: 0.7939 - val_loss: 0.4906\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7959 - loss: 0.4877 - val_accuracy: 0.7938 - val_loss: 0.4911\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7964 - loss: 0.4879 - val_accuracy: 0.7938 - val_loss: 0.4911\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7956 - loss: 0.4882 - val_accuracy: 0.7937 - val_loss: 0.4911\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7964 - loss: 0.4874 - val_accuracy: 0.7939 - val_loss: 0.4907\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 713us/step - accuracy: 0.7962 - loss: 0.4880 - val_accuracy: 0.7938 - val_loss: 0.4903\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7955 - loss: 0.4886 - val_accuracy: 0.7939 - val_loss: 0.4904\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7968 - loss: 0.4864 - val_accuracy: 0.7940 - val_loss: 0.4907\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7967 - loss: 0.4870 - val_accuracy: 0.7937 - val_loss: 0.4910\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7962 - loss: 0.4877 - val_accuracy: 0.7939 - val_loss: 0.4910\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7968 - loss: 0.4861 - val_accuracy: 0.7940 - val_loss: 0.4918\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7950 - loss: 0.4888 - val_accuracy: 0.7939 - val_loss: 0.4908\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7958 - loss: 0.4881 - val_accuracy: 0.7938 - val_loss: 0.4907\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7973 - loss: 0.4859 - val_accuracy: 0.7939 - val_loss: 0.4907\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7959 - loss: 0.4876 - val_accuracy: 0.7939 - val_loss: 0.4910\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7968 - loss: 0.4857 - val_accuracy: 0.7939 - val_loss: 0.4907\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7966 - loss: 0.4865 - val_accuracy: 0.7939 - val_loss: 0.4921\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7973 - loss: 0.4861 - val_accuracy: 0.7939 - val_loss: 0.4902\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7957 - loss: 0.4881 - val_accuracy: 0.7937 - val_loss: 0.4908\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7968 - loss: 0.4869 - val_accuracy: 0.7937 - val_loss: 0.4910\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637us/step\n",
      "Accuracy for fold 2: 0.7937282525672579\n",
      "Training on fold 3...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 717us/step - accuracy: 0.7952 - loss: 0.4985 - val_accuracy: 0.7951 - val_loss: 0.4919\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7955 - loss: 0.4932 - val_accuracy: 0.7953 - val_loss: 0.4895\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 702us/step - accuracy: 0.7960 - loss: 0.4917 - val_accuracy: 0.7952 - val_loss: 0.4918\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 707us/step - accuracy: 0.7946 - loss: 0.4922 - val_accuracy: 0.7953 - val_loss: 0.4900\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 705us/step - accuracy: 0.7954 - loss: 0.4902 - val_accuracy: 0.7952 - val_loss: 0.4893\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 707us/step - accuracy: 0.7963 - loss: 0.4893 - val_accuracy: 0.7954 - val_loss: 0.4892\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7963 - loss: 0.4894 - val_accuracy: 0.7952 - val_loss: 0.4883\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7950 - loss: 0.4912 - val_accuracy: 0.7952 - val_loss: 0.4890\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7963 - loss: 0.4887 - val_accuracy: 0.7952 - val_loss: 0.4883\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 708us/step - accuracy: 0.7952 - loss: 0.4904 - val_accuracy: 0.7951 - val_loss: 0.4884\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7944 - loss: 0.4912 - val_accuracy: 0.7956 - val_loss: 0.4890\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7957 - loss: 0.4898 - val_accuracy: 0.7953 - val_loss: 0.4881\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7956 - loss: 0.4896 - val_accuracy: 0.7952 - val_loss: 0.4882\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7972 - loss: 0.4878 - val_accuracy: 0.7952 - val_loss: 0.4877\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7970 - loss: 0.4874 - val_accuracy: 0.7953 - val_loss: 0.4879\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7961 - loss: 0.4884 - val_accuracy: 0.7951 - val_loss: 0.4881\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7946 - loss: 0.4904 - val_accuracy: 0.7951 - val_loss: 0.4884\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7960 - loss: 0.4888 - val_accuracy: 0.7952 - val_loss: 0.4878\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7964 - loss: 0.4881 - val_accuracy: 0.7952 - val_loss: 0.4874\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7958 - loss: 0.4889 - val_accuracy: 0.7951 - val_loss: 0.4874\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7962 - loss: 0.4882 - val_accuracy: 0.7952 - val_loss: 0.4879\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7954 - loss: 0.4896 - val_accuracy: 0.7952 - val_loss: 0.4875\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7957 - loss: 0.4886 - val_accuracy: 0.7952 - val_loss: 0.4880\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7955 - loss: 0.4894 - val_accuracy: 0.7952 - val_loss: 0.4878\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7950 - loss: 0.4895 - val_accuracy: 0.7951 - val_loss: 0.4878\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7953 - loss: 0.4899 - val_accuracy: 0.7951 - val_loss: 0.4873\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7957 - loss: 0.4895 - val_accuracy: 0.7952 - val_loss: 0.4871\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7972 - loss: 0.4875 - val_accuracy: 0.7951 - val_loss: 0.4874\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7978 - loss: 0.4862 - val_accuracy: 0.7952 - val_loss: 0.4875\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7955 - loss: 0.4895 - val_accuracy: 0.7957 - val_loss: 0.4871\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7969 - loss: 0.4872 - val_accuracy: 0.7952 - val_loss: 0.4872\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7964 - loss: 0.4880 - val_accuracy: 0.7951 - val_loss: 0.4871\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7965 - loss: 0.4871 - val_accuracy: 0.7952 - val_loss: 0.4877\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7958 - loss: 0.4884 - val_accuracy: 0.7952 - val_loss: 0.4885\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7962 - loss: 0.4880 - val_accuracy: 0.7951 - val_loss: 0.4884\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7973 - loss: 0.4865 - val_accuracy: 0.7953 - val_loss: 0.4875\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7968 - loss: 0.4870 - val_accuracy: 0.7952 - val_loss: 0.4878\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7965 - loss: 0.4884 - val_accuracy: 0.7955 - val_loss: 0.4876\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7966 - loss: 0.4875 - val_accuracy: 0.7950 - val_loss: 0.4873\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7967 - loss: 0.4877 - val_accuracy: 0.7952 - val_loss: 0.4882\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7945 - loss: 0.4900 - val_accuracy: 0.7950 - val_loss: 0.4878\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7958 - loss: 0.4881 - val_accuracy: 0.7953 - val_loss: 0.4878\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7972 - loss: 0.4865 - val_accuracy: 0.7953 - val_loss: 0.4880\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7961 - loss: 0.4877 - val_accuracy: 0.7952 - val_loss: 0.4871\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7957 - loss: 0.4885 - val_accuracy: 0.7952 - val_loss: 0.4871\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7949 - loss: 0.4894 - val_accuracy: 0.7953 - val_loss: 0.4893\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7951 - loss: 0.4892 - val_accuracy: 0.7952 - val_loss: 0.4876\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7960 - loss: 0.4881 - val_accuracy: 0.7952 - val_loss: 0.4872\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7956 - loss: 0.4881 - val_accuracy: 0.7953 - val_loss: 0.4875\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7963 - loss: 0.4875 - val_accuracy: 0.7952 - val_loss: 0.4874\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step\n",
      "Accuracy for fold 3: 0.7952134430959857\n",
      "Training on fold 4...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 729us/step - accuracy: 0.7894 - loss: 0.5010 - val_accuracy: 0.7963 - val_loss: 0.4945\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 713us/step - accuracy: 0.7949 - loss: 0.4937 - val_accuracy: 0.7961 - val_loss: 0.4923\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7963 - loss: 0.4894 - val_accuracy: 0.7957 - val_loss: 0.4920\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7972 - loss: 0.4881 - val_accuracy: 0.7960 - val_loss: 0.4906\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 713us/step - accuracy: 0.7959 - loss: 0.4902 - val_accuracy: 0.7958 - val_loss: 0.4900\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7947 - loss: 0.4912 - val_accuracy: 0.7962 - val_loss: 0.4902\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7962 - loss: 0.4892 - val_accuracy: 0.7962 - val_loss: 0.4905\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7959 - loss: 0.4894 - val_accuracy: 0.7960 - val_loss: 0.4903\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7954 - loss: 0.4894 - val_accuracy: 0.7961 - val_loss: 0.4903\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7979 - loss: 0.4859 - val_accuracy: 0.7961 - val_loss: 0.4902\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7965 - loss: 0.4886 - val_accuracy: 0.7962 - val_loss: 0.4905\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7965 - loss: 0.4877 - val_accuracy: 0.7962 - val_loss: 0.4902\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7967 - loss: 0.4880 - val_accuracy: 0.7961 - val_loss: 0.4903\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7936 - loss: 0.4920 - val_accuracy: 0.7959 - val_loss: 0.4909\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7962 - loss: 0.4875 - val_accuracy: 0.7963 - val_loss: 0.4895\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7953 - loss: 0.4896 - val_accuracy: 0.7961 - val_loss: 0.4899\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7959 - loss: 0.4889 - val_accuracy: 0.7962 - val_loss: 0.4910\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7937 - loss: 0.4912 - val_accuracy: 0.7962 - val_loss: 0.4899\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7940 - loss: 0.4908 - val_accuracy: 0.7961 - val_loss: 0.4892\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7965 - loss: 0.4874 - val_accuracy: 0.7963 - val_loss: 0.4903\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7974 - loss: 0.4869 - val_accuracy: 0.7960 - val_loss: 0.4897\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7966 - loss: 0.4875 - val_accuracy: 0.7963 - val_loss: 0.4897\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7962 - loss: 0.4883 - val_accuracy: 0.7964 - val_loss: 0.4909\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7956 - loss: 0.4891 - val_accuracy: 0.7958 - val_loss: 0.4889\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7951 - loss: 0.4889 - val_accuracy: 0.7964 - val_loss: 0.4899\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7950 - loss: 0.4898 - val_accuracy: 0.7962 - val_loss: 0.4899\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7952 - loss: 0.4893 - val_accuracy: 0.7964 - val_loss: 0.4894\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7953 - loss: 0.4894 - val_accuracy: 0.7963 - val_loss: 0.4898\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7962 - loss: 0.4877 - val_accuracy: 0.7964 - val_loss: 0.4905\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7959 - loss: 0.4882 - val_accuracy: 0.7961 - val_loss: 0.4892\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7944 - loss: 0.4898 - val_accuracy: 0.7962 - val_loss: 0.4896\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7956 - loss: 0.4887 - val_accuracy: 0.7959 - val_loss: 0.4888\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7952 - loss: 0.4889 - val_accuracy: 0.7960 - val_loss: 0.4902\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7959 - loss: 0.4876 - val_accuracy: 0.7962 - val_loss: 0.4889\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7971 - loss: 0.4866 - val_accuracy: 0.7959 - val_loss: 0.4886\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7964 - loss: 0.4873 - val_accuracy: 0.7962 - val_loss: 0.4890\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7960 - loss: 0.4882 - val_accuracy: 0.7963 - val_loss: 0.4886\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7951 - loss: 0.4892 - val_accuracy: 0.7964 - val_loss: 0.4918\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7985 - loss: 0.4841 - val_accuracy: 0.7960 - val_loss: 0.4887\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7973 - loss: 0.4862 - val_accuracy: 0.7964 - val_loss: 0.4890\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7975 - loss: 0.4856 - val_accuracy: 0.7963 - val_loss: 0.4896\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7966 - loss: 0.4864 - val_accuracy: 0.7960 - val_loss: 0.4885\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7948 - loss: 0.4886 - val_accuracy: 0.7961 - val_loss: 0.4883\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7957 - loss: 0.4879 - val_accuracy: 0.7964 - val_loss: 0.4885\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7957 - loss: 0.4873 - val_accuracy: 0.7962 - val_loss: 0.4886\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7946 - loss: 0.4901 - val_accuracy: 0.7962 - val_loss: 0.4886\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7971 - loss: 0.4861 - val_accuracy: 0.7966 - val_loss: 0.4892\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7972 - loss: 0.4853 - val_accuracy: 0.7961 - val_loss: 0.4888\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7970 - loss: 0.4875 - val_accuracy: 0.7963 - val_loss: 0.4888\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7959 - loss: 0.4881 - val_accuracy: 0.7961 - val_loss: 0.4884\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step\n",
      "Accuracy for fold 4: 0.7961469914283289\n",
      "Training on fold 5...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 723us/step - accuracy: 0.7949 - loss: 0.4990 - val_accuracy: 0.7964 - val_loss: 0.4923\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7947 - loss: 0.4930 - val_accuracy: 0.7966 - val_loss: 0.4902\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7943 - loss: 0.4929 - val_accuracy: 0.7956 - val_loss: 0.4892\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7968 - loss: 0.4893 - val_accuracy: 0.7957 - val_loss: 0.4898\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 711us/step - accuracy: 0.7953 - loss: 0.4908 - val_accuracy: 0.7962 - val_loss: 0.4907\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7956 - loss: 0.4898 - val_accuracy: 0.7962 - val_loss: 0.4893\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 713us/step - accuracy: 0.7963 - loss: 0.4889 - val_accuracy: 0.7964 - val_loss: 0.4904\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7945 - loss: 0.4911 - val_accuracy: 0.7961 - val_loss: 0.4890\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7968 - loss: 0.4879 - val_accuracy: 0.7962 - val_loss: 0.4884\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7957 - loss: 0.4890 - val_accuracy: 0.7964 - val_loss: 0.4889\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7970 - loss: 0.4880 - val_accuracy: 0.7962 - val_loss: 0.4898\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7957 - loss: 0.4887 - val_accuracy: 0.7962 - val_loss: 0.4892\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7958 - loss: 0.4896 - val_accuracy: 0.7963 - val_loss: 0.4886\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7951 - loss: 0.4903 - val_accuracy: 0.7964 - val_loss: 0.4888\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7956 - loss: 0.4895 - val_accuracy: 0.7965 - val_loss: 0.4890\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7941 - loss: 0.4916 - val_accuracy: 0.7963 - val_loss: 0.4887\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7968 - loss: 0.4874 - val_accuracy: 0.7964 - val_loss: 0.4889\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7963 - loss: 0.4876 - val_accuracy: 0.7964 - val_loss: 0.4885\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7945 - loss: 0.4902 - val_accuracy: 0.7963 - val_loss: 0.4892\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7964 - loss: 0.4883 - val_accuracy: 0.7962 - val_loss: 0.4886\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7953 - loss: 0.4893 - val_accuracy: 0.7965 - val_loss: 0.4894\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7956 - loss: 0.4888 - val_accuracy: 0.7964 - val_loss: 0.4882\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7958 - loss: 0.4887 - val_accuracy: 0.7964 - val_loss: 0.4889\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7956 - loss: 0.4896 - val_accuracy: 0.7961 - val_loss: 0.4883\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7934 - loss: 0.4920 - val_accuracy: 0.7964 - val_loss: 0.4893\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7986 - loss: 0.4853 - val_accuracy: 0.7961 - val_loss: 0.4885\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7965 - loss: 0.4869 - val_accuracy: 0.7964 - val_loss: 0.4888\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7962 - loss: 0.4878 - val_accuracy: 0.7961 - val_loss: 0.4890\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7976 - loss: 0.4861 - val_accuracy: 0.7963 - val_loss: 0.4889\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7962 - loss: 0.4883 - val_accuracy: 0.7962 - val_loss: 0.4885\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7953 - loss: 0.4889 - val_accuracy: 0.7963 - val_loss: 0.4881\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7969 - loss: 0.4867 - val_accuracy: 0.7966 - val_loss: 0.4882\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7964 - loss: 0.4879 - val_accuracy: 0.7962 - val_loss: 0.4882\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7962 - loss: 0.4882 - val_accuracy: 0.7964 - val_loss: 0.4896\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7951 - loss: 0.4895 - val_accuracy: 0.7963 - val_loss: 0.4885\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7965 - loss: 0.4878 - val_accuracy: 0.7962 - val_loss: 0.4884\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7962 - loss: 0.4876 - val_accuracy: 0.7963 - val_loss: 0.4885\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7941 - loss: 0.4906 - val_accuracy: 0.7963 - val_loss: 0.4893\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7941 - loss: 0.4903 - val_accuracy: 0.7963 - val_loss: 0.4889\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7973 - loss: 0.4863 - val_accuracy: 0.7964 - val_loss: 0.4879\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7972 - loss: 0.4866 - val_accuracy: 0.7964 - val_loss: 0.4883\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7963 - loss: 0.4873 - val_accuracy: 0.7960 - val_loss: 0.4885\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7945 - loss: 0.4897 - val_accuracy: 0.7964 - val_loss: 0.4881\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7958 - loss: 0.4888 - val_accuracy: 0.7962 - val_loss: 0.4888\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7957 - loss: 0.4887 - val_accuracy: 0.7964 - val_loss: 0.4887\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7936 - loss: 0.4909 - val_accuracy: 0.7959 - val_loss: 0.4884\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7960 - loss: 0.4874 - val_accuracy: 0.7962 - val_loss: 0.4882\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7944 - loss: 0.4898 - val_accuracy: 0.7963 - val_loss: 0.4883\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7957 - loss: 0.4886 - val_accuracy: 0.7963 - val_loss: 0.4893\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7969 - loss: 0.4869 - val_accuracy: 0.7963 - val_loss: 0.4891\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step\n",
      "Accuracy for fold 5: 0.7962656482070868\n",
      "Training on fold 6...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733us/step - accuracy: 0.7938 - loss: 0.4979 - val_accuracy: 0.7961 - val_loss: 0.4939\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 699us/step - accuracy: 0.7936 - loss: 0.4948 - val_accuracy: 0.7961 - val_loss: 0.4896\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7960 - loss: 0.4907 - val_accuracy: 0.7967 - val_loss: 0.4904\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 707us/step - accuracy: 0.7959 - loss: 0.4903 - val_accuracy: 0.7969 - val_loss: 0.4884\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 710us/step - accuracy: 0.7944 - loss: 0.4915 - val_accuracy: 0.7968 - val_loss: 0.4890\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7962 - loss: 0.4886 - val_accuracy: 0.7969 - val_loss: 0.4896\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 709us/step - accuracy: 0.7963 - loss: 0.4896 - val_accuracy: 0.7969 - val_loss: 0.4886\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7964 - loss: 0.4892 - val_accuracy: 0.7969 - val_loss: 0.4885\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7957 - loss: 0.4892 - val_accuracy: 0.7970 - val_loss: 0.4883\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7954 - loss: 0.4901 - val_accuracy: 0.7968 - val_loss: 0.4886\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7968 - loss: 0.4880 - val_accuracy: 0.7969 - val_loss: 0.4884\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7945 - loss: 0.4909 - val_accuracy: 0.7969 - val_loss: 0.4887\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7954 - loss: 0.4906 - val_accuracy: 0.7971 - val_loss: 0.4879\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7969 - loss: 0.4881 - val_accuracy: 0.7970 - val_loss: 0.4886\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7956 - loss: 0.4889 - val_accuracy: 0.7970 - val_loss: 0.4887\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7967 - loss: 0.4875 - val_accuracy: 0.7969 - val_loss: 0.4885\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7951 - loss: 0.4903 - val_accuracy: 0.7969 - val_loss: 0.4883\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7964 - loss: 0.4874 - val_accuracy: 0.7969 - val_loss: 0.4885\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7982 - loss: 0.4866 - val_accuracy: 0.7968 - val_loss: 0.4888\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7965 - loss: 0.4877 - val_accuracy: 0.7967 - val_loss: 0.4877\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7961 - loss: 0.4881 - val_accuracy: 0.7969 - val_loss: 0.4877\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7941 - loss: 0.4916 - val_accuracy: 0.7968 - val_loss: 0.4878\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7956 - loss: 0.4890 - val_accuracy: 0.7967 - val_loss: 0.4879\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7948 - loss: 0.4899 - val_accuracy: 0.7968 - val_loss: 0.4890\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7954 - loss: 0.4892 - val_accuracy: 0.7968 - val_loss: 0.4883\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7951 - loss: 0.4900 - val_accuracy: 0.7968 - val_loss: 0.4879\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7962 - loss: 0.4886 - val_accuracy: 0.7967 - val_loss: 0.4881\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7962 - loss: 0.4875 - val_accuracy: 0.7966 - val_loss: 0.4887\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7964 - loss: 0.4873 - val_accuracy: 0.7967 - val_loss: 0.4876\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7955 - loss: 0.4885 - val_accuracy: 0.7968 - val_loss: 0.4878\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7954 - loss: 0.4890 - val_accuracy: 0.7969 - val_loss: 0.4879\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7951 - loss: 0.4893 - val_accuracy: 0.7966 - val_loss: 0.4878\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7959 - loss: 0.4878 - val_accuracy: 0.7968 - val_loss: 0.4877\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7949 - loss: 0.4899 - val_accuracy: 0.7970 - val_loss: 0.4873\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7955 - loss: 0.4881 - val_accuracy: 0.7964 - val_loss: 0.4875\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7963 - loss: 0.4876 - val_accuracy: 0.7967 - val_loss: 0.4878\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7959 - loss: 0.4876 - val_accuracy: 0.7967 - val_loss: 0.4880\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7954 - loss: 0.4880 - val_accuracy: 0.7967 - val_loss: 0.4877\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7978 - loss: 0.4854 - val_accuracy: 0.7969 - val_loss: 0.4872\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7955 - loss: 0.4889 - val_accuracy: 0.7970 - val_loss: 0.4875\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7964 - loss: 0.4874 - val_accuracy: 0.7971 - val_loss: 0.4874\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7951 - loss: 0.4890 - val_accuracy: 0.7965 - val_loss: 0.4872\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7966 - loss: 0.4872 - val_accuracy: 0.7968 - val_loss: 0.4877\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7966 - loss: 0.4868 - val_accuracy: 0.7966 - val_loss: 0.4877\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7962 - loss: 0.4880 - val_accuracy: 0.7964 - val_loss: 0.4875\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7966 - loss: 0.4870 - val_accuracy: 0.7967 - val_loss: 0.4873\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7960 - loss: 0.4872 - val_accuracy: 0.7971 - val_loss: 0.4890\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7967 - loss: 0.4871 - val_accuracy: 0.7969 - val_loss: 0.4876\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7950 - loss: 0.4884 - val_accuracy: 0.7966 - val_loss: 0.4880\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7958 - loss: 0.4885 - val_accuracy: 0.7965 - val_loss: 0.4867\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step\n",
      "Accuracy for fold 6: 0.7964778272862296\n",
      "Training on fold 7...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - accuracy: 0.7960 - loss: 0.4980 - val_accuracy: 0.7964 - val_loss: 0.4907\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 711us/step - accuracy: 0.7962 - loss: 0.4910 - val_accuracy: 0.7964 - val_loss: 0.4895\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 711us/step - accuracy: 0.7946 - loss: 0.4927 - val_accuracy: 0.7965 - val_loss: 0.4881\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7965 - loss: 0.4894 - val_accuracy: 0.7965 - val_loss: 0.4877\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7955 - loss: 0.4905 - val_accuracy: 0.7966 - val_loss: 0.4905\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7954 - loss: 0.4910 - val_accuracy: 0.7965 - val_loss: 0.4876\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7946 - loss: 0.4912 - val_accuracy: 0.7967 - val_loss: 0.4883\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7950 - loss: 0.4906 - val_accuracy: 0.7967 - val_loss: 0.4888\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7942 - loss: 0.4916 - val_accuracy: 0.7966 - val_loss: 0.4874\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7964 - loss: 0.4885 - val_accuracy: 0.7966 - val_loss: 0.4875\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7938 - loss: 0.4919 - val_accuracy: 0.7970 - val_loss: 0.4902\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7964 - loss: 0.4882 - val_accuracy: 0.7967 - val_loss: 0.4874\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7968 - loss: 0.4880 - val_accuracy: 0.7968 - val_loss: 0.4869\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7953 - loss: 0.4895 - val_accuracy: 0.7966 - val_loss: 0.4875\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7958 - loss: 0.4892 - val_accuracy: 0.7967 - val_loss: 0.4868\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7971 - loss: 0.4877 - val_accuracy: 0.7968 - val_loss: 0.4868\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7961 - loss: 0.4878 - val_accuracy: 0.7968 - val_loss: 0.4901\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7950 - loss: 0.4903 - val_accuracy: 0.7965 - val_loss: 0.4876\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7941 - loss: 0.4914 - val_accuracy: 0.7965 - val_loss: 0.4868\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7953 - loss: 0.4899 - val_accuracy: 0.7966 - val_loss: 0.4870\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7961 - loss: 0.4884 - val_accuracy: 0.7969 - val_loss: 0.4868\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 742us/step - accuracy: 0.7952 - loss: 0.4890 - val_accuracy: 0.7961 - val_loss: 0.4869\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7964 - loss: 0.4876 - val_accuracy: 0.7966 - val_loss: 0.4867\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7976 - loss: 0.4856 - val_accuracy: 0.7968 - val_loss: 0.4870\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 749us/step - accuracy: 0.7962 - loss: 0.4878 - val_accuracy: 0.7964 - val_loss: 0.4869\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7973 - loss: 0.4860 - val_accuracy: 0.7966 - val_loss: 0.4869\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7964 - loss: 0.4871 - val_accuracy: 0.7965 - val_loss: 0.4868\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7958 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4868\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7959 - loss: 0.4885 - val_accuracy: 0.7967 - val_loss: 0.4865\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7940 - loss: 0.4901 - val_accuracy: 0.7965 - val_loss: 0.4876\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 745us/step - accuracy: 0.7965 - loss: 0.4879 - val_accuracy: 0.7964 - val_loss: 0.4864\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7936 - loss: 0.4919 - val_accuracy: 0.7967 - val_loss: 0.4864\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7962 - loss: 0.4877 - val_accuracy: 0.7964 - val_loss: 0.4871\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7969 - loss: 0.4862 - val_accuracy: 0.7967 - val_loss: 0.4862\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7962 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4863\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 750us/step - accuracy: 0.7974 - loss: 0.4866 - val_accuracy: 0.7968 - val_loss: 0.4865\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7939 - loss: 0.4912 - val_accuracy: 0.7966 - val_loss: 0.4871\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7947 - loss: 0.4898 - val_accuracy: 0.7966 - val_loss: 0.4868\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7964 - loss: 0.4874 - val_accuracy: 0.7968 - val_loss: 0.4867\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7949 - loss: 0.4898 - val_accuracy: 0.7964 - val_loss: 0.4869\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7960 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4862\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7961 - loss: 0.4872 - val_accuracy: 0.7969 - val_loss: 0.4866\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7955 - loss: 0.4889 - val_accuracy: 0.7965 - val_loss: 0.4867\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7954 - loss: 0.4885 - val_accuracy: 0.7964 - val_loss: 0.4865\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - accuracy: 0.7955 - loss: 0.4885 - val_accuracy: 0.7969 - val_loss: 0.4861\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7965 - loss: 0.4872 - val_accuracy: 0.7968 - val_loss: 0.4863\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7958 - loss: 0.4876 - val_accuracy: 0.7964 - val_loss: 0.4869\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 742us/step - accuracy: 0.7955 - loss: 0.4883 - val_accuracy: 0.7968 - val_loss: 0.4863\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 748us/step - accuracy: 0.7953 - loss: 0.4884 - val_accuracy: 0.7965 - val_loss: 0.4861\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7954 - loss: 0.4879 - val_accuracy: 0.7966 - val_loss: 0.4867\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635us/step\n",
      "Accuracy for fold 7: 0.7965626989178867\n",
      "Training on fold 8...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 736us/step - accuracy: 0.7946 - loss: 0.4991 - val_accuracy: 0.7946 - val_loss: 0.4942\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7962 - loss: 0.4915 - val_accuracy: 0.7946 - val_loss: 0.4939\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7956 - loss: 0.4905 - val_accuracy: 0.7945 - val_loss: 0.4921\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7951 - loss: 0.4908 - val_accuracy: 0.7947 - val_loss: 0.4921\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7943 - loss: 0.4921 - val_accuracy: 0.7947 - val_loss: 0.4915\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7974 - loss: 0.4874 - val_accuracy: 0.7948 - val_loss: 0.4941\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7957 - loss: 0.4891 - val_accuracy: 0.7949 - val_loss: 0.4916\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7955 - loss: 0.4893 - val_accuracy: 0.7952 - val_loss: 0.4917\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7955 - loss: 0.4895 - val_accuracy: 0.7952 - val_loss: 0.4912\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7960 - loss: 0.4889 - val_accuracy: 0.7950 - val_loss: 0.4912\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7952 - loss: 0.4895 - val_accuracy: 0.7950 - val_loss: 0.4912\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7959 - loss: 0.4892 - val_accuracy: 0.7947 - val_loss: 0.4907\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7961 - loss: 0.4888 - val_accuracy: 0.7950 - val_loss: 0.4918\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7955 - loss: 0.4892 - val_accuracy: 0.7948 - val_loss: 0.4914\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7972 - loss: 0.4873 - val_accuracy: 0.7951 - val_loss: 0.4911\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7951 - loss: 0.4899 - val_accuracy: 0.7949 - val_loss: 0.4906\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7968 - loss: 0.4873 - val_accuracy: 0.7947 - val_loss: 0.4912\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7974 - loss: 0.4865 - val_accuracy: 0.7949 - val_loss: 0.4911\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7950 - loss: 0.4897 - val_accuracy: 0.7948 - val_loss: 0.4919\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7961 - loss: 0.4883 - val_accuracy: 0.7950 - val_loss: 0.4911\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7963 - loss: 0.4881 - val_accuracy: 0.7948 - val_loss: 0.4914\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7958 - loss: 0.4885 - val_accuracy: 0.7950 - val_loss: 0.4911\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7954 - loss: 0.4893 - val_accuracy: 0.7949 - val_loss: 0.4909\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7954 - loss: 0.4897 - val_accuracy: 0.7949 - val_loss: 0.4910\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7958 - loss: 0.4893 - val_accuracy: 0.7946 - val_loss: 0.4907\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 741us/step - accuracy: 0.7969 - loss: 0.4872 - val_accuracy: 0.7947 - val_loss: 0.4913\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7966 - loss: 0.4876 - val_accuracy: 0.7947 - val_loss: 0.4905\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7965 - loss: 0.4878 - val_accuracy: 0.7947 - val_loss: 0.4922\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7949 - loss: 0.4901 - val_accuracy: 0.7946 - val_loss: 0.4904\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7961 - loss: 0.4878 - val_accuracy: 0.7945 - val_loss: 0.4923\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7970 - loss: 0.4870 - val_accuracy: 0.7948 - val_loss: 0.4909\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7965 - loss: 0.4880 - val_accuracy: 0.7950 - val_loss: 0.4914\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7955 - loss: 0.4889 - val_accuracy: 0.7950 - val_loss: 0.4914\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7965 - loss: 0.4875 - val_accuracy: 0.7949 - val_loss: 0.4909\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7959 - loss: 0.4881 - val_accuracy: 0.7949 - val_loss: 0.4904\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 736us/step - accuracy: 0.7961 - loss: 0.4885 - val_accuracy: 0.7949 - val_loss: 0.4907\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7978 - loss: 0.4859 - val_accuracy: 0.7944 - val_loss: 0.4908\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7953 - loss: 0.4890 - val_accuracy: 0.7949 - val_loss: 0.4906\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7966 - loss: 0.4864 - val_accuracy: 0.7950 - val_loss: 0.4905\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7973 - loss: 0.4868 - val_accuracy: 0.7947 - val_loss: 0.4902\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7967 - loss: 0.4868 - val_accuracy: 0.7947 - val_loss: 0.4907\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7943 - loss: 0.4894 - val_accuracy: 0.7948 - val_loss: 0.4905\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 735us/step - accuracy: 0.7955 - loss: 0.4891 - val_accuracy: 0.7947 - val_loss: 0.4901\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7967 - loss: 0.4873 - val_accuracy: 0.7948 - val_loss: 0.4907\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7950 - loss: 0.4887 - val_accuracy: 0.7947 - val_loss: 0.4905\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 733us/step - accuracy: 0.7948 - loss: 0.4894 - val_accuracy: 0.7943 - val_loss: 0.4908\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 742us/step - accuracy: 0.7960 - loss: 0.4881 - val_accuracy: 0.7950 - val_loss: 0.4906\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - accuracy: 0.7970 - loss: 0.4854 - val_accuracy: 0.7947 - val_loss: 0.4902\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7961 - loss: 0.4878 - val_accuracy: 0.7945 - val_loss: 0.4901\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 740us/step - accuracy: 0.7950 - loss: 0.4886 - val_accuracy: 0.7945 - val_loss: 0.4901\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630us/step\n",
      "Accuracy for fold 8: 0.7944833439422873\n",
      "Training on fold 9...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 720us/step - accuracy: 0.7943 - loss: 0.4991 - val_accuracy: 0.7950 - val_loss: 0.4928\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7952 - loss: 0.4921 - val_accuracy: 0.7950 - val_loss: 0.4922\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 714us/step - accuracy: 0.7962 - loss: 0.4905 - val_accuracy: 0.7953 - val_loss: 0.4918\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7958 - loss: 0.4901 - val_accuracy: 0.7954 - val_loss: 0.4910\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7972 - loss: 0.4882 - val_accuracy: 0.7955 - val_loss: 0.4909\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720us/step - accuracy: 0.7947 - loss: 0.4909 - val_accuracy: 0.7957 - val_loss: 0.4905\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7955 - loss: 0.4901 - val_accuracy: 0.7958 - val_loss: 0.4904\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.7960 - loss: 0.4894 - val_accuracy: 0.7958 - val_loss: 0.4904\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7946 - loss: 0.4905 - val_accuracy: 0.7958 - val_loss: 0.4900\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7955 - loss: 0.4897 - val_accuracy: 0.7958 - val_loss: 0.4901\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7945 - loss: 0.4907 - val_accuracy: 0.7958 - val_loss: 0.4899\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7949 - loss: 0.4900 - val_accuracy: 0.7959 - val_loss: 0.4908\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7961 - loss: 0.4888 - val_accuracy: 0.7958 - val_loss: 0.4900\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7952 - loss: 0.4895 - val_accuracy: 0.7959 - val_loss: 0.4898\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7952 - loss: 0.4898 - val_accuracy: 0.7959 - val_loss: 0.4899\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - accuracy: 0.7978 - loss: 0.4862 - val_accuracy: 0.7959 - val_loss: 0.4893\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7960 - loss: 0.4890 - val_accuracy: 0.7958 - val_loss: 0.4903\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7961 - loss: 0.4890 - val_accuracy: 0.7959 - val_loss: 0.4899\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7973 - loss: 0.4870 - val_accuracy: 0.7958 - val_loss: 0.4894\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7950 - loss: 0.4903 - val_accuracy: 0.7958 - val_loss: 0.4899\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7963 - loss: 0.4881 - val_accuracy: 0.7958 - val_loss: 0.4891\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7954 - loss: 0.4894 - val_accuracy: 0.7958 - val_loss: 0.4897\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7953 - loss: 0.4893 - val_accuracy: 0.7959 - val_loss: 0.4914\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7963 - loss: 0.4885 - val_accuracy: 0.7958 - val_loss: 0.4893\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7972 - loss: 0.4867 - val_accuracy: 0.7958 - val_loss: 0.4894\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7957 - loss: 0.4892 - val_accuracy: 0.7958 - val_loss: 0.4898\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7964 - loss: 0.4886 - val_accuracy: 0.7958 - val_loss: 0.4899\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 739us/step - accuracy: 0.7951 - loss: 0.4895 - val_accuracy: 0.7959 - val_loss: 0.4891\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7962 - loss: 0.4878 - val_accuracy: 0.7957 - val_loss: 0.4890\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7963 - loss: 0.4880 - val_accuracy: 0.7958 - val_loss: 0.4897\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7955 - loss: 0.4894 - val_accuracy: 0.7958 - val_loss: 0.4890\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7960 - loss: 0.4885 - val_accuracy: 0.7958 - val_loss: 0.4897\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7973 - loss: 0.4860 - val_accuracy: 0.7958 - val_loss: 0.4894\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7967 - loss: 0.4870 - val_accuracy: 0.7956 - val_loss: 0.4889\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7962 - loss: 0.4881 - val_accuracy: 0.7958 - val_loss: 0.4890\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7951 - loss: 0.4895 - val_accuracy: 0.7958 - val_loss: 0.4890\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7969 - loss: 0.4870 - val_accuracy: 0.7958 - val_loss: 0.4891\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7961 - loss: 0.4877 - val_accuracy: 0.7959 - val_loss: 0.4895\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7961 - loss: 0.4880 - val_accuracy: 0.7957 - val_loss: 0.4890\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7953 - loss: 0.4890 - val_accuracy: 0.7959 - val_loss: 0.4894\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7958 - loss: 0.4885 - val_accuracy: 0.7955 - val_loss: 0.4894\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7957 - loss: 0.4887 - val_accuracy: 0.7957 - val_loss: 0.4893\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7952 - loss: 0.4892 - val_accuracy: 0.7959 - val_loss: 0.4898\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7954 - loss: 0.4891 - val_accuracy: 0.7955 - val_loss: 0.4890\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7944 - loss: 0.4898 - val_accuracy: 0.7955 - val_loss: 0.4890\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7957 - loss: 0.4886 - val_accuracy: 0.7958 - val_loss: 0.4887\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7974 - loss: 0.4866 - val_accuracy: 0.7956 - val_loss: 0.4889\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7949 - loss: 0.4890 - val_accuracy: 0.7957 - val_loss: 0.4889\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7948 - loss: 0.4894 - val_accuracy: 0.7955 - val_loss: 0.4891\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7963 - loss: 0.4868 - val_accuracy: 0.7955 - val_loss: 0.4892\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step\n",
      "Accuracy for fold 9: 0.7955442393380012\n",
      "Training on fold 10...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 721us/step - accuracy: 0.7953 - loss: 0.4975 - val_accuracy: 0.7965 - val_loss: 0.4907\n",
      "Epoch 2/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 708us/step - accuracy: 0.7967 - loss: 0.4904 - val_accuracy: 0.7965 - val_loss: 0.4914\n",
      "Epoch 3/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 710us/step - accuracy: 0.7948 - loss: 0.4925 - val_accuracy: 0.7965 - val_loss: 0.4898\n",
      "Epoch 4/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 711us/step - accuracy: 0.7957 - loss: 0.4903 - val_accuracy: 0.7966 - val_loss: 0.4887\n",
      "Epoch 5/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 708us/step - accuracy: 0.7968 - loss: 0.4887 - val_accuracy: 0.7967 - val_loss: 0.4891\n",
      "Epoch 6/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712us/step - accuracy: 0.7959 - loss: 0.4897 - val_accuracy: 0.7966 - val_loss: 0.4888\n",
      "Epoch 7/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 717us/step - accuracy: 0.7962 - loss: 0.4895 - val_accuracy: 0.7963 - val_loss: 0.4880\n",
      "Epoch 8/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 709us/step - accuracy: 0.7968 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4880\n",
      "Epoch 9/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7960 - loss: 0.4893 - val_accuracy: 0.7962 - val_loss: 0.4882\n",
      "Epoch 10/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7959 - loss: 0.4890 - val_accuracy: 0.7964 - val_loss: 0.4880\n",
      "Epoch 11/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7945 - loss: 0.4907 - val_accuracy: 0.7963 - val_loss: 0.4882\n",
      "Epoch 12/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7967 - loss: 0.4877 - val_accuracy: 0.7961 - val_loss: 0.4880\n",
      "Epoch 13/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7958 - loss: 0.4895 - val_accuracy: 0.7963 - val_loss: 0.4883\n",
      "Epoch 14/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 716us/step - accuracy: 0.7952 - loss: 0.4902 - val_accuracy: 0.7964 - val_loss: 0.4885\n",
      "Epoch 15/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7936 - loss: 0.4923 - val_accuracy: 0.7964 - val_loss: 0.4883\n",
      "Epoch 16/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7944 - loss: 0.4909 - val_accuracy: 0.7963 - val_loss: 0.4879\n",
      "Epoch 17/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7953 - loss: 0.4900 - val_accuracy: 0.7964 - val_loss: 0.4881\n",
      "Epoch 18/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723us/step - accuracy: 0.7961 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4878\n",
      "Epoch 19/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7939 - loss: 0.4910 - val_accuracy: 0.7964 - val_loss: 0.4881\n",
      "Epoch 20/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7970 - loss: 0.4874 - val_accuracy: 0.7962 - val_loss: 0.4889\n",
      "Epoch 21/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7963 - loss: 0.4876 - val_accuracy: 0.7964 - val_loss: 0.4882\n",
      "Epoch 22/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 734us/step - accuracy: 0.7964 - loss: 0.4883 - val_accuracy: 0.7965 - val_loss: 0.4881\n",
      "Epoch 23/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719us/step - accuracy: 0.7936 - loss: 0.4913 - val_accuracy: 0.7963 - val_loss: 0.4875\n",
      "Epoch 24/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 731us/step - accuracy: 0.7962 - loss: 0.4882 - val_accuracy: 0.7964 - val_loss: 0.4874\n",
      "Epoch 25/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7973 - loss: 0.4864 - val_accuracy: 0.7964 - val_loss: 0.4879\n",
      "Epoch 26/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7954 - loss: 0.4894 - val_accuracy: 0.7965 - val_loss: 0.4881\n",
      "Epoch 27/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7954 - loss: 0.4891 - val_accuracy: 0.7967 - val_loss: 0.4873\n",
      "Epoch 28/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7963 - loss: 0.4880 - val_accuracy: 0.7964 - val_loss: 0.4875\n",
      "Epoch 29/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7966 - loss: 0.4876 - val_accuracy: 0.7965 - val_loss: 0.4877\n",
      "Epoch 30/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7959 - loss: 0.4877 - val_accuracy: 0.7964 - val_loss: 0.4882\n",
      "Epoch 31/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7964 - loss: 0.4874 - val_accuracy: 0.7963 - val_loss: 0.4878\n",
      "Epoch 32/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7946 - loss: 0.4900 - val_accuracy: 0.7966 - val_loss: 0.4876\n",
      "Epoch 33/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718us/step - accuracy: 0.7952 - loss: 0.4892 - val_accuracy: 0.7964 - val_loss: 0.4886\n",
      "Epoch 34/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7960 - loss: 0.4885 - val_accuracy: 0.7963 - val_loss: 0.4873\n",
      "Epoch 35/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7962 - loss: 0.4881 - val_accuracy: 0.7964 - val_loss: 0.4888\n",
      "Epoch 36/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7964 - loss: 0.4871 - val_accuracy: 0.7963 - val_loss: 0.4900\n",
      "Epoch 37/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7964 - loss: 0.4883 - val_accuracy: 0.7964 - val_loss: 0.4876\n",
      "Epoch 38/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7959 - loss: 0.4879 - val_accuracy: 0.7962 - val_loss: 0.4898\n",
      "Epoch 39/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - accuracy: 0.7967 - loss: 0.4875 - val_accuracy: 0.7963 - val_loss: 0.4874\n",
      "Epoch 40/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7966 - loss: 0.4874 - val_accuracy: 0.7964 - val_loss: 0.4879\n",
      "Epoch 41/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - accuracy: 0.7946 - loss: 0.4895 - val_accuracy: 0.7965 - val_loss: 0.4877\n",
      "Epoch 42/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 726us/step - accuracy: 0.7983 - loss: 0.4858 - val_accuracy: 0.7964 - val_loss: 0.4877\n",
      "Epoch 43/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 732us/step - accuracy: 0.7965 - loss: 0.4868 - val_accuracy: 0.7965 - val_loss: 0.4872\n",
      "Epoch 44/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 722us/step - accuracy: 0.7982 - loss: 0.4852 - val_accuracy: 0.7964 - val_loss: 0.4882\n",
      "Epoch 45/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7956 - loss: 0.4880 - val_accuracy: 0.7964 - val_loss: 0.4903\n",
      "Epoch 46/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 729us/step - accuracy: 0.7957 - loss: 0.4878 - val_accuracy: 0.7965 - val_loss: 0.4872\n",
      "Epoch 47/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - accuracy: 0.7956 - loss: 0.4880 - val_accuracy: 0.7965 - val_loss: 0.4872\n",
      "Epoch 48/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 725us/step - accuracy: 0.7965 - loss: 0.4868 - val_accuracy: 0.7965 - val_loss: 0.4871\n",
      "Epoch 49/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 728us/step - accuracy: 0.7954 - loss: 0.4884 - val_accuracy: 0.7964 - val_loss: 0.4883\n",
      "Epoch 50/50\n",
      "\u001b[1m6628/6628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 724us/step - accuracy: 0.7962 - loss: 0.4871 - val_accuracy: 0.7966 - val_loss: 0.4871\n",
      "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635us/step\n",
      "Accuracy for fold 10: 0.7966475705495438\n",
      "\n",
      "Final Report:\n",
      "Mean Accuracy: 0.795755647888179\n",
      "Standard Deviation of Accuracy: 0.0009478138442350506\n",
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n",
      "\n",
      "Classification Report for fold 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.02      4796\n",
      "         1.0       0.80      1.00      0.89     18769\n",
      "\n",
      "    accuracy                           0.80     23565\n",
      "   macro avg       0.66      0.50      0.45     23565\n",
      "weighted avg       0.74      0.80      0.71     23565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_deep_learning_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4feac83c3b4f80bd90bdcc8f159a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_automl_kfold(processed_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def train_svm_kfold(processed_data, k=5):\n",
    "    df = processed_data.copy()\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    \n",
    "    X = df.drop('Status', axis=1)\n",
    "    y = df['Status']\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    reports = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training on fold {fold}...\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Khởi tạo và huấn luyện mô hình SVM trên tập train\n",
    "        model = SVC(probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Dự đoán trên tập test\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Đánh giá mô hình\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold}: {accuracy}\")\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracies)}\")\n",
    "    \n",
    "    for i, report in enumerate(reports):\n",
    "        print(f\"\\nClassification Report for fold {i+1}:\")\n",
    "        print(classification_report(y.iloc[test_index], y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm_kfold(processed_data, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
